% -*- mode: LaTeX; compile-command: "./build.sh" -*-

\documentclass[nolinenum]{jfp}

% \usepackage{showkeys}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% lhs2TeX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\let\Bbbk\undefined  % https://github.com/kosmikus/lhs2tex/issues/82
%% ODER: format ==         = "\mathrel{==}"
%% ODER: format /=         = "\neq "
%
%
\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}%
  {\@namedef{lhs2tex.lhs2tex.sty.read}{}%
   \newcommand\SkipToFmtEnd{}%
   \newcommand\EndFmtInput{}%
   \long\def\SkipToFmtEnd#1\EndFmtInput{}%
  }\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	% NEU

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}% suggested by Neil Mitchell
\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

%mathindent has to be defined
\@ifundefined{mathindent}%
  {\newdimen\mathindent\mathindent\leftmargini}%
  {}%

\def\resethooks{%
  \global\let\SaveRestoreHook\empty
  \global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]%
  {\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{%
  \let\onelinecomment=\onelinecommentchars
  \let\commentbegin=\commentbeginchars
  \let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{%
  \let\onelinecomment=\empty
  \let\commentbegin=\empty
  \let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\hsindent}[1]{\quad}% default is fixed indentation
\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{$\langle$\textbf{To do:}~#1$\rangle$}

\EndFmtInput
\makeatother
%
%
%
%
%
%
% This package provides two environments suitable to take the place
% of hscode, called "plainhscode" and "arrayhscode". 
%
% The plain environment surrounds each code block by vertical space,
% and it uses \abovedisplayskip and \belowdisplayskip to get spacing
% similar to formulas. Note that if these dimensions are changed,
% the spacing around displayed math formulas changes as well.
% All code is indented using \leftskip.
%
% Changed 19.08.2004 to reflect changes in colorcode. Should work with
% CodeGroup.sty.
%
\ReadOnlyOnce{polycode.fmt}%
\makeatletter

\newcommand{\hsnewpar}[1]%
  {{\parskip=0pt\parindent=0pt\par\vskip #1\noindent}}

% can be used, for instance, to redefine the code size, by setting the
% command to \small or something alike
\newcommand{\hscodestyle}{}

% The command \sethscode can be used to switch the code formatting
% behaviour by mapping the hscode environment in the subst directive
% to a new LaTeX environment.

\newcommand{\sethscode}[1]%
  {\expandafter\let\expandafter\hscode\csname #1\endcsname
   \expandafter\let\expandafter\endhscode\csname end#1\endcsname}

% "compatibility" mode restores the non-polycode.fmt layout.

\newenvironment{compathscode}%
  {\par\noindent
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \let\hspre\(\let\hspost\)%
   \pboxed}%
  {\endpboxed\)%
   \par\noindent
   \ignorespacesafterend}

\newcommand{\compaths}{\sethscode{compathscode}}

% "plain" mode is the proposed default.
% It should now work with \centering.
% This required some changes. The old version
% is still available for reference as oldplainhscode.

\newenvironment{plainhscode}%
  {\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\hspre\(\let\hspost\)%
   \pboxed}%
  {\endpboxed%
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newenvironment{oldplainhscode}%
  {\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \(\pboxed}%
  {\endpboxed\)%
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

% Here, we make plainhscode the default environment.

\newcommand{\plainhs}{\sethscode{plainhscode}}
\newcommand{\oldplainhs}{\sethscode{oldplainhscode}}
\plainhs

% The arrayhscode is like plain, but makes use of polytable's
% parray environment which disallows page breaks in code blocks.

\newenvironment{arrayhscode}%
  {\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \(\parray}%
  {\endparray\)%
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newcommand{\arrayhs}{\sethscode{arrayhscode}}

% The mathhscode environment also makes use of polytable's parray 
% environment. It is supposed to be used only inside math mode 
% (I used it to typeset the type rules in my thesis).

\newenvironment{mathhscode}%
  {\parray}{\endparray}

\newcommand{\mathhs}{\sethscode{mathhscode}}

% texths is similar to mathhs, but works in text mode.

\newenvironment{texthscode}%
  {\(\parray}{\endparray\)}

\newcommand{\texths}{\sethscode{texthscode}}

% The framed environment places code in a framed box.

\def\codeframewidth{\arrayrulewidth}
\RequirePackage{calc}

\newenvironment{framedhscode}%
  {\parskip=\abovedisplayskip\par\noindent
   \hscodestyle
   \arrayrulewidth=\codeframewidth
   \tabular{@{}|p{\linewidth-2\arraycolsep-2\arrayrulewidth-2pt}|@{}}%
   \hline\framedhslinecorrect\\{-1.5ex}%
   \let\endoflinesave=\\
   \let\\=\@normalcr
   \(\pboxed}%
  {\endpboxed\)%
   \framedhslinecorrect\endoflinesave{.5ex}\hline
   \endtabular
   \parskip=\belowdisplayskip\par\noindent
   \ignorespacesafterend}

\newcommand{\framedhslinecorrect}[2]%
  {#1[#2]}

\newcommand{\framedhs}{\sethscode{framedhscode}}

% The inlinehscode environment is an experimental environment
% that can be used to typeset displayed code inline.

\newenvironment{inlinehscode}%
  {\(\def\column##1##2{}%
   \let\>\undefined\let\<\undefined\let\\\undefined
   \newcommand\>[1][]{}\newcommand\<[1][]{}\newcommand\\[1][]{}%
   \def\fromto##1##2##3{##3}%
   \def\nextline{}}{\) }%

\newcommand{\inlinehs}{\sethscode{inlinehscode}}

% The joincode environment is a separate environment that
% can be used to surround and thereby connect multiple code
% blocks.

\newenvironment{joincode}%
  {\let\orighscode=\hscode
   \let\origendhscode=\endhscode
   \def\endhscode{\def\hscode{\endgroup\def\@currenvir{hscode}\\}\begingroup}
   %\let\SaveRestoreHook=\empty
   %\let\ColumnHook=\empty
   %\let\resethooks=\empty
   \orighscode\def\hscode{\endgroup\def\@currenvir{hscode}}}%
  {\origendhscode
   \global\let\hscode=\orighscode
   \global\let\endhscode=\origendhscode}%

\makeatother
\EndFmtInput
%













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{booktabs}   %% For formal tables: http://ctan.org/pkg/booktabs
\usepackage{todonotes}
\usepackage{enumitem}
\newcommand{\todoi}[2][]{\todo[inline, #1]{#2}}

\usepackage{xspace}
\usepackage{prettyref}
\usepackage{bbm}
\usepackage{stmaryrd}

\usepackage{minted}

\usepackage{thmtools, thm-restate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Prettyref

\newrefformat{fig}{Figure~\ref{#1}}
\newrefformat{sec}{Section~\ref{#1}}
\newrefformat{eq}{equation~\eqref{#1}}
\newrefformat{prob}{Problem~\ref{#1}}
\newrefformat{tab}{Table~\ref{#1}}
\newrefformat{thm}{Theorem~\ref{#1}}
\newrefformat{lem}{Lemma~\ref{#1}}
\newrefformat{claim}{Claim~\ref{#1}}
\newrefformat{obs}{Observation~\ref{#1}}
\newrefformat{prop}{Proposition~\ref{#1}}
\newrefformat{defn}{Definition~\ref{#1}}
\newrefformat{cor}{Corollary~\ref{#1}}
\providecommand{\pref}{}
\renewcommand{\pref}[1]{\prettyref{#1}}

% \Pref is just like \pref but it uppercases the first letter; for use
% at the beginning of a sentence.
\providecommand{\Pref}{}
\renewcommand{\Pref}[1]{%
  \expandafter\ifx\csname r@#1\endcsname\relax {\scriptsize[ref]}
    \else
    \edef\reftext{\prettyref{#1}}\expandafter\MakeUppercase\reftext
    \fi
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% structured proofs
\newenvironment{sproof}{%
    \begin{tabbing}
    \phantom{$\equiv$} \= \qquad\qquad\qquad\qquad\qquad \= \kill
}{
    \end{tabbing}
}
\newcommand{\stmt}[1]{\> \ensuremath{#1} \\}
\newcommand{\lstmt}[1]{\> \ensuremath{#1} }
\newcommand{\reason}[2]{\ensuremath{#1} \>\> \{ \quad #2 \quad \} \\}

\newcommand{\subpart}[1]{\llcorner #1 \lrcorner}
\newcommand{\suppart}[1]{\ulcorner #1 \urcorner}

%%% Other math stuff

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}[thm]{Claim}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{obs}{Observation}
\newtheorem{prob}{Problem}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{ex}{Example}
\newtheorem*{nota}{Notation}

\newcommand{\bits}{\ensuremath{\mathbbm{2}}}

\newcommand{\mempty}{0}

\newcommand{\Up}{\textbf{U}\xspace}
\newcommand{\RQ}{\textbf{RQ}\xspace}

\newcommand{\ie}{\emph{i.e.}\xspace}

\newcommand{\term}[1]{\emph{#1}}

\newcommand{\interleaveop}{\curlyvee}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\sem}[1]{\llbracket {#1} \rrbracket}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newif\ifJFP
\JFPtrue

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\journaltitle{Journal of Functional Programming}
\cpr{Cambridge University Press}
\doival{10.1017/xxxxx}

\jnlDoiYr{2024}

\title{You Could Have Invented Fenwick Trees}
\begin{authgrp}
  \author{Brent A. Yorgey}
  \affiliation{Hendrix College \\ 1600 Washington Ave, Conway, AR
    72032, USA \\ \email{yorgey@hendrix.edu} }
\end{authgrp}

\begin{abstract}
  \emph{Fenwick trees}, also known as \emph{binary indexed trees}, are
  a clever solution to the problem of maintaining a sequence of values
  while allowing both updates and range queries in sublinear time.
  Their implementation is concise and efficient---but also somewhat
  baffling, consisting largely of nonobvious bitwise operations on
  indices.  We begin with \emph{segment trees}, a much more
  straightforward, easy-to-verify, purely functional solution to the
  problem, and use equational reasoning to explain the implementation
  of Fenwick trees as an optimized variant, making use of a Haskell
  EDSL for operations on infinite two's complement binary numbers.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
%        <concept_desc>Software and its engineering~Functional languages</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%    <concept>
%        <concept_id>10003752.10003809.10010031</concept_id>
%        <concept_desc>Theory of computation~Data structures design and analysis</concept_desc>
%        <concept_significance>300</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Functional languages}
% \ccsdesc[300]{Theory of computation~Data structures design and analysis}
%% End of generated code


% \begin{keywords}
%   keyword1, keyword2, keyword3
% \end{keywords}

\maketitle[F]

\section{Introduction}
\label{sec:intro}

Suppose we have a sequence of $n$ integers $a_1, a_2, \dots, a_n$, and
want to be able to perform arbitrary interleavings of the following
two operations, illustrated in \pref{fig:update-rq}:

\begin{itemize}
\item \emph{Update} the value at any given index\footnote{Note that we
    use $1$-based indexing here and throughout the paper, that is, the
    first item in the sequence has index $1$.  The reasons for this
    choice will become clear later.} $i$ by adding some value $v$.
\item Find the sum of all values in any given range $[i, j]$, that
  is, $a_i + a_{i+1} + \dots + a_j$.  We call this operation a
  \emph{range query}.
\end{itemize}
Note that update is phrased in terms of \emph{adding} some value $v$
to the existing value; we can also \emph{set} a given index to a new value
$v$ by adding $v - u$, where $u$ is the old value.

If we simply store the integers in a mutable array, then we can update
in constant time, but range queries require time linear in the size
of the range, since we must iterate through the entire range $[i, j]$
to add up the values.

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig1.pgf}
\end{center}
\caption{Update and range query operations} \label{fig:update-rq}
\end{figure}

In order to improve the running time of range queries, we could try to
cache (at least some of) the range sums.  However, this must be done
with care, since the cached sums must be kept up to date when updating
the value at an index.  For example, a straightforward approach would
be to use an array $P$ where $P_i$ stores the prefix sum
$a_1 + \dots + a_i$; $P$ can be precomputed in linear time via a scan.
Now range queries are fast: we can obtain $a_i + \dots + a_j$ in
constant time by computing $P_j - P_{i-1}$ (for convenience we set
$P_0 = 0$ so this works even when $i=1$).  Unfortunately, it is update
that now takes linear time, since changing $a_i$ requires updating
$P_j$ for every $j \geq i$.

Is it possible to design a data structure that allows \emph{both}
operations to run in sublinear time?  (You may wish to pause and think
about it before reading the next paragraph!)  This is not just
academic: the problem was originally considered in the context of
\emph{arithmetic coding} \citep{rissanen1979arithmetic,
  bird2002arithmetic}, a family of techniques for turning messages
into sequences of bits for storage or transmission.  In order to
minimize the bits required, one generally wants to assign shorter bit
sequences to more frequent characters, and vice versa; this leads to
the need to maintain a dynamic table of character frequencies.  We
\emph{update} the table every time a new character is processed, and
\emph{query} the table for cumulative frequencies in order to
subdivide a unit interval into consecutive segments proportional to
the frequency of each character \citep{fenwick1994new, ryabko1989fast}.

So, can we get both operations to run in sublinear time?  The answer,
of course, is yes.  One simple technique is to divide the sequence
into $\sqrt n$ buckets, each of size $\sqrt n$, and create an
additional array of size $\sqrt n$ to cache the sum of each bucket.
Updates still run in $O(1)$, since we simply have to update the value
at the given index and the corresponding bucket sum.
Range queries now run in $O(\sqrt n)$ time: to find the sum
$a_i + \dots + a_j$, we manually add the values from $a_i$ to the end
of its bucket, and from $a_j$ to the beginning of its bucket; for all
the buckets in between we can just look up their sum.

We can make range queries even faster, at the cost of making updates
slightly slower, by introducing additional levels of caching.  For
example, we can divide the sequence into $\sqrt[3] n$ ``big buckets'',
and then further subdivide each big bucket into $\sqrt[3] n$ ``small
buckets'', with each small bucket holding $\sqrt[3] n$ values.  The
sum of each bucket is cached; now each update requires modifying three
values, and range queries run in $O(\sqrt[3] n)$ time.

In the limit, we end up with a binary divide-and-conquer approach to
caching range sums, with both update and range query taking $O(\lg n)$
time.  In particular, we can make a balanced binary tree where the
leaves store the sequence itself, and every internal node stores the
sum of its children.  (This will be a familiar idea to many functional
programmers; for example, finger trees
\citep{Hinze-Paterson:FingerTree, apfelmus:fingertree} use a similar sort of caching scheme.)  The
resulting data structure is popularly known as a \emph{segment
  tree}\footnote{There is some confusion of terminology here.  As of
  this writing, the Wikipedia article on \emph{segment trees}
  \citep{wiki:SegmentTree} is about an interval data structure used in
  computational geometry.  However, most of the Google search results
  for ``segment tree'' are from the world of competitive programming,
  where it refers to the data structure considered in this paper (see,
  for example, \citet[\S 2.8]{CP4} or \citet{cp-alg}). The two data structures are largely
  unrelated.}, presumably because each internal node ultimately caches
the sum of a (contiguous) \emph{segment} of the underlying sequence.
\pref{fig:segment-tree} shows a segment tree built on a sample array
of length $n=16$ (for simplicity, we will assume that $n$ is a power
of two, although it is easy to generalize to situations where it is
not). Each leaf of the tree corresponds to an array entry; each
internal node is drawn with a grey bar showing the segment of the
underlying array of which it is the sum.

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig2.pgf}
\end{center}
\caption{A segment tree} \label{fig:segment-tree}
\end{figure}

Let's see how we can use a segment tree to implement the two required
operations so that they run in logarithmic time.

\begin{itemize}
\item To update the value at index $i$, we also need to update any
  cached range sums which include it.  These are exactly the nodes
  along the path from the leaf at index $i$ to the root of the tree;
  there are $O(\lg n)$ such nodes.  \pref{fig:segment-tree-update}
  illustrates this update process for the example segment tree from
  \pref{fig:segment-tree}; updating the entry at index 5 requires
  modifying only the shaded nodes along the path from the root to the
  updated entry.

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig3.pgf}
\end{center}
\caption{Updating a segment tree} \label{fig:segment-tree-update}
\end{figure}

\item To perform a range query, we descend through the tree while
  keeping track of the range covered by the current node.
  \begin{itemize}
  \item If the range of the current node is wholly contained within
    the query range, return the value of the current
    node.
  \item If the range of the current node is disjoint from the query
    range, return $0$.
  \item Otherwise, recursively query both children and return the sum of the
    results.
  \end{itemize}
\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig4.pgf}
\end{center}
\caption{Performing a range query on a segment tree} \label{fig:segment-tree-range-query}
\end{figure}
\pref{fig:segment-tree-range-query} illustrates the process of
computing the sum of the range $[4 \dots 11]$.  Blue nodes are the
ones we recurse through; green nodes are those whose range is wholly
contained in the query range, and are returned without recursing
further; grey nodes are disjoint from the query range and return zero.
The final result in this example is the sum of values at the green
nodes, $1 + 1 + 5 + -2 = 5$ (it is easily verified that this is in
fact the sum of values in the range $[4 \dots 11]$).

On this small example tree, it may seem that we visit a significant
fraction of the total nodes, but in general, we visit no more than
about $4 \lg n$.  \pref{fig:big-range-query} makes this more
clear.  Only one blue node in the entire tree can have two blue
children, and hence each level of the tree can contain at most two
blue nodes and two non-blue nodes. We essentially perform two binary
searches, one to find each endpoint of the query range.
\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig5.pgf}
\end{center}
\caption{Performing a range query on a larger segment tree} \label{fig:big-range-query}
\end{figure}
\end{itemize}

Segment trees are a very nice solution to the problem: as we will see
in \pref{sec:seg-trees}, they fit well in a functional language;
they also lend themselves to powerful generalizations such as lazily
propagated range updates and persistent update history via shared
immutable structure \citep{cp-alg}.

\term{Fenwick trees}, or \term{binary indexed trees}
\citep{fenwick1994new, cp-alg-fenwick}, are an alternative solution to the problem.
What they lack in generality, they make up for with an extremely small
memory footprint---they require literally nothing more than an array
storing the values in the tree---and a blazing fast implementation.
In other words, they are perfect for applications such as low-level
coding/decoding routines where we don't need any of the advanced
features that segment trees offer, and want to squeeze out every last
bit of performance.

\pref{fig:fenwick-java} shows a typical implementation of Fenwick
trees in Java. As you can see, the implementation is incredibly
concise, and consists mostly of some small loops doing just a few
arithmetic and bit operations per iteration.  It is not at all clear
what this code is doing, or how it works!  Upon closer inspection, the
\mintinline{java}{range}, \mintinline{java}{get}, and
\mintinline{java}{set} functions are straightforward, but the other
functions are a puzzle. We can see that both the
\mintinline{java}{prefix} and \mintinline{java}{update} functions call
another function \mintinline{java}{LSB}, which for some reason
performs a bitwise logical AND of an integer and its negation.  In
fact, \mintinline{java}{LSB(x)} computes the \emph{least significant
  bit} of $x$, that is, it returns the smallest $2^k$ such that the
$k$th bit of $x$ is a one.  However, it is not obvious how the
implementation of \mintinline{java}{LSB} works, nor how and why least
significant bits are being used to compute updates and prefix sums.

\begin{figure}
  \inputminted[fontsize=\footnotesize]{java}{FenwickTree.java}
  \caption{Implementing Fenwick trees with bit tricks}
  \label{fig:fenwick-java}
\end{figure}

Our goal is \emph{not} to write elegant functional code for
this---already solved!---problem.  Rather, our goal will be to use a
functional domain-specific language for bit strings, along with
equational reasoning, to \emph{derive} and \emph{explain} this
baffling imperative code from first principles---a demonstration of
the power of functional thinking and equational reasoning to
understand code written even in other, non-functional languages.
After developing more intuition for segment trees
(\pref{sec:seg-trees}), we will see how Fenwick trees can be viewed as
a variant on segment trees (\pref{sec:fenwick}).  We will then take a
detour into two's complement binary encoding, develop a suitable DSL
for bit manipuations, and explain the implementation of the
\mintinline{java}{LSB} function (\pref{sec:twos-complement}).  Armed
with the DSL, we will then derive functions for converting back and
forth between Fenwick trees and standard binary trees
(\pref{sec:convert}).  Finally, we will be able to derive functions
for moving within a Fenwick tree by converting to binary tree indices,
doing the obvious operations to effect the desired motion within the
binary tree, and then converting back.  Fusing away the conversions
via equational reasoning will finally reveal the hidden LSB function,
as expected (\pref{sec:fenwick-ops}).

This paper was produced from a literate Haskell document; the source
is available from GitHub, at \url{https://github.com/byorgey/fenwick/blob/master/Fenwick.lhs}.

\section{Segment Trees}
\label{sec:seg-trees}

\pref{fig:haskell-segtree} exhibits a simple implementation of a
segment tree in Haskell, using some utilities for working with index
ranges shown in \pref{fig:ranges}.  We store a segment tree as a
recursive algebraic data type, and implement \ensuremath{\Varid{update}} and \ensuremath{\Varid{rq}} using
code that directly corresponds to the recursive descriptions given in
the previous section; \ensuremath{\Varid{get}} and \ensuremath{\Varid{set}} can then also be implemented in
terms of them.  It is not hard to generalize this code to work for
segment trees storing values from either an arbitrary commutative
monoid if we don't need the \ensuremath{\Varid{set}} operation---or from an arbitrary
Abelian group (\ie commutative monoid with inverses) if we do need
\ensuremath{\Varid{set}}---but we keep things simple since the generalization doesn't add
anything to our story.

\begin{figure}
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{Index}\mathrel{=}\Conid{Int}{}\<[E]%
\\
\>[B]{}\mathbf{data}\;\Conid{Range}\mathrel{=}\Conid{Index}\mathrel{:\!\text{---}\!:}\Conid{Index}{}\<[34]%
\>[34]{}\mbox{\onelinecomment  ($a$ \ensuremath{\mathrel{:\!\text{---}\!:}} $b$) represents the closed interval $[a,b]$}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{deriving}\;(\Conid{Eq},\Conid{Show}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}(\subseteq)\mathbin{::}\Conid{Range}\to \Conid{Range}\to \Conid{Bool}{}\<[E]%
\\
\>[B]{}(\Varid{lo}_{1}\mathrel{:\!\text{---}\!:}\Varid{hi}_{1})\subseteq(\Varid{lo}_{2}\mathrel{:\!\text{---}\!:}\Varid{hi}_{2})\mathrel{=}\Varid{lo}_{2}\leq \Varid{lo}_{1}\mathrel{\wedge}\Varid{hi}_{1}\leq \Varid{hi}_{2}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}(\in)\mathbin{::}\Conid{Index}\to \Conid{Range}\to \Conid{Bool}{}\<[E]%
\\
\>[B]{}\Varid{k}\in\Varid{i}\mathrel{=}(\Varid{k}\mathrel{:\!\text{---}\!:}\Varid{k})\subseteq\Varid{i}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{disjoint}\mathbin{::}\Conid{Range}\to \Conid{Range}\to \Conid{Bool}{}\<[E]%
\\
\>[B]{}\Varid{disjoint}\;(\Varid{lo}_{1}\mathrel{:\!\text{---}\!:}\Varid{hi}_{1})\;(\Varid{lo}_{2}\mathrel{:\!\text{---}\!:}\Varid{hi}_{2})\mathrel{=}\Varid{hi}_{1}\mathbin{<}\Varid{lo}_{2}\mathrel{\vee}\Varid{hi}_{2}\mathbin{<}\Varid{lo}_{1}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
  \caption{Range utilities}
  \label{fig:ranges}
\end{figure}

\begin{figure}
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{SegTree}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{Empty}{}\<[11]%
\>[11]{}\mathbin{::}\Conid{SegTree}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{Branch}{}\<[11]%
\>[11]{}\mathbin{::}\Conid{Integer}\to \Conid{Range}\to \Conid{SegTree}\to \Conid{SegTree}\to \Conid{SegTree}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{update}\mathbin{::}\Conid{Index}\to \Conid{Integer}\to \Conid{SegTree}\to \Conid{SegTree}{}\<[E]%
\\
\>[B]{}\Varid{update}\;\anonymous \;\anonymous \;\Conid{Empty}\mathrel{=}\Conid{Empty}{}\<[E]%
\\
\>[B]{}\Varid{update}\;\Varid{i}\;\Varid{v}\;\Varid{b}\mathord{@}(\Conid{Branch}\;\Varid{a}\;\Varid{rng}\;\Varid{l}\;\Varid{r}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{i}\in\Varid{rng}{}\<[18]%
\>[18]{}\mathrel{=}\Conid{Branch}\;(\Varid{a}\mathbin{+}\Varid{v})\;\Varid{rng}\;(\Varid{update}\;\Varid{i}\;\Varid{v}\;\Varid{l})\;(\Varid{update}\;\Varid{i}\;\Varid{v}\;\Varid{r}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{otherwise}{}\<[18]%
\>[18]{}\mathrel{=}\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{rq}\mathbin{::}\Conid{Range}\to \Conid{SegTree}\to \Conid{Integer}{}\<[E]%
\\
\>[B]{}\Varid{rq}\;\anonymous \;\Conid{Empty}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[B]{}\Varid{rq}\;\Varid{q}\;(\Conid{Branch}\;\Varid{a}\;\Varid{rng}\;\Varid{l}\;\Varid{r}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{disjoint}\;\Varid{rng}\;\Varid{q}{}\<[21]%
\>[21]{}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{rng}\subseteq\Varid{q}{}\<[21]%
\>[21]{}\mathrel{=}\Varid{a}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{otherwise}{}\<[21]%
\>[21]{}\mathrel{=}\Varid{rq}\;\Varid{q}\;\Varid{l}\mathbin{+}\Varid{rq}\;\Varid{q}\;\Varid{r}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{get}\mathbin{::}\Conid{Index}\to \Conid{SegTree}\to \Conid{Integer}{}\<[E]%
\\
\>[B]{}\Varid{get}\;\Varid{i}\mathrel{=}\Varid{rq}\;(\Varid{i}\mathrel{:\!\text{---}\!:}\Varid{i}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{set}\mathbin{::}\Conid{Index}\to \Conid{Integer}\to \Conid{SegTree}\to \Conid{SegTree}{}\<[E]%
\\
\>[B]{}\Varid{set}\;\Varid{i}\;\Varid{v}\;\Varid{t}\mathrel{=}\Varid{update}\;\Varid{i}\;(\Varid{v}\mathbin{-}\Varid{get}\;\Varid{i}\;\Varid{t})\;\Varid{t}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Simple segment tree implementation in Haskell} \label{fig:haskell-segtree}
\end{figure}

Although this implementation is simple and relatively straightforward
to understand, compared to simply storing the sequence of values in an
array, it incurs a good deal of overhead.  We can be more clever in
our use of space by storing all the nodes of a segment tree in an
array, using the standard left-to-right breadth-first indexing scheme
illustrated in \pref{fig:bt-indexing} (for example, this scheme, or
something like it, is commonly used to implement binary heaps).  The
root has label $1$; every time we descend one level we append an extra
bit: $0$ when we descend to the left child and $1$ when we descend to
the right.  Thus, the index of each node expressed in binary records
the sequence of left-right choices along the path to that node from
the root.  Going from a node to its children is as simple as doing a
left bit-shift and optionally adding 1; going from a node to its
parent is a right bit-shift.  This defines a bijection from the
positive natural numbers to the nodes of an infinite binary tree.  If
we label the segment tree array with $s_1 \dots s_{2n-1}$, then $s_1$
stores the sum of all the $a_i$, $s_2$ stores the sum of the first
half of the $a_i$, $s_3$ stores the sum of the second half, and so on.
$a_1 \dots a_n$ themselves are stored as $s_n \dots s_{2n-1}$.

\begin{figure}
  \centering
  \input{diagrams/Fenwick-diagrams-latex-fig6.pgf}
  \caption{Indexing a binary tree}
  \label{fig:bt-indexing}
\end{figure}

The important point is that since descending recursively through the
tree corresponds to simple operations on indices, all the algorithms
we have discussed can be straightforwardly transformed into
code that works with a (mutable) array: for example, instead of storing
a reference to the current subtree, we store an integer index; every
time we want to descend to the left or right we simply double the
current index or double and add one; and so on.  Working with tree
nodes stored in an array presents an additional opportunity: rather
than being forced to start at the root and recurse downwards, we can
start at a particular index of interest and move \emph{up} the tree
instead.

So how do we get from segment trees to Fenwick trees?  We start with
an innocuous-seeming observation: \emph{not all the values stored in a
  segment tree are necessary}.  Of course, all the non-leaf nodes are
``unnecessary'' in the sense that they represent cached range sums
which could easily be recomputed from the original sequence.  That's
the whole point: caching these ``redundant'' sums trades off space for
time, allowing us to perform arbitrary updates and range queries
quickly, at the cost of doubling the required storage space.

But that's not what I mean! In fact, there is a different set of
values we can forget about, but in such a way that we still retain the
logarithmic running time for updates and range queries. Which values,
you ask?  Simple: just forget the data stored in \emph{every node
  which is a right child}. \pref{fig:deactivate-right} shows the same
example tree we have been using, but with the data deleted from every
right child.  Note that ``every right child'' includes both leaves and
internal nodes: we forget the data associated to \emph{every} node
which is the right child of its parent.  We will refer to the nodes
with discarded data as \emph{inactive} and the remaining nodes (that
is, left children and the root) as \emph{active}.  We also say that a
tree with all its right children inactivated in this way has been
\emph{thinned}.

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig7.pgf}
\end{center}
\caption{Inactivating all right children in a segment tree} \label{fig:deactivate-right}
\end{figure}

Updating a thinned segment tree is easy: just update the same nodes as
before, ignoring any updates to inactive nodes.  But how do we answer
range queries?  It's not too hard to see that there is enough
information remaining to reconstruct the information that was
discarded (you might like to try convincing yourself of this: can you
deduce what values must go in the greyed-out nodes in
\pref{fig:deactivate-right}, without peeking at any previous
figures?).  However, in and of itself, this observation does not give
us a nice algorithm for computing range sums.

It turns out the key is to think about \emph{prefix sums}.  As we saw
in the introduction and the implementation of
\mintinline{java}{range} in \pref{fig:fenwick-java}, if we can compute
the prefix sum $P_k = a_1 + \dots + a_k$ for any $k$, then we can
compute the range sum $a_i + \dots + a_j$ as $P_j - P_{i-1}$.

\begin{theorem}
  Given a thinned segment tree, the sum of \emph{any prefix} of the
  original array (and hence also any range sum) can be computed, in
  logarithmic time, using only the values of active nodes.
\end{theorem}
\begin{proof}
  Surprisingly, in the special case of prefix queries, the original
  range query algorithm described in \pref{sec:intro} and implemented
  in \pref{fig:haskell-segtree} works unchanged!  That is to say, the
  base case in which the range of the current node is wholly contained
  within the query range---and we thus return the value of the current
  node---will only ever happen at active nodes.

  First, the root itself is active, and hence querying the full range
  will work.  Next, consider the case where we are at a node and
  recurse on both children.  The left child is always active, so we
  only need to consider the case where we recurse to the right.  It is
  impossible that the range of the right child will be wholly
  contained in the query range: since the query range is always a
  prefix of the form $[1,j]$, if the right child's range is wholly
  contained in $[1,j]$ then the left child's range must be as
  well---which means that the parent node's range (which is the union
  of its children's ranges) would also be wholly contained in the
  query range.  But in that case we would simply return the parent's
  value without recursing into the right child.  Thus, when we do
  recurse into a right child, we might end up returning $0$, or we
  might recurse further into both grandchildren, but in any case we
  will never try to look at the value of the right child itself.
\end{proof}

\pref{fig:segment-tree-prefix-query} illustrates performing a prefix
query on a segment tree.  Notice that visited right children are only ever
blue or grey; the only green nodes are left children.
\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig8.pgf}
\end{center}
\caption{Performing a prefix query on a segment tree} \label{fig:segment-tree-prefix-query}
\end{figure}

\section{Fenwick trees}
\label{sec:fenwick}

How should we actually store a thinned segment tree in memory?  If we
stare at \pref{fig:deactivate-right} again, one strategy suggests
itself: simply take every active node and ``slide'' it down and to the
right until it lands in an empty slot in the underlying array, as
illustrated in \pref{fig:sliding-right}.  This sets up a one-to-one
correspondence between active nodes and indices in the range
$1 \dots n$.  Another way to understand this indexing scheme is to use
a postorder traversal of the tree, skipping over inactive nodes and
giving consecutive indices to active nodes encountered during the
traversal.  We can also visualize the result by drawing the tree in a
``right-leaning'' style (\pref{fig:right-leaning}), vertically
aligning each active node with the array slot where it is stored.

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig9.pgf}
\end{center}
\caption{Sliding active values down a thinned segment tree} \label{fig:sliding-right}
\end{figure}

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig10.pgf}
\end{center}
\caption{Right-leaning drawing of a thinned segment tree, vertically
  aligning nodes with their storage
  location} \label{fig:right-leaning}
\end{figure}

This method of storing the active nodes from a thinned segment tree in
an array is precisely a \emph{Fenwick tree}. I will also sometimes
refer to it as a \emph{Fenwick array}, when I want to particularly
emphasize the underlying array data structure.  Although it is
certainly a clever use of space, the big question is how to implement
the update and range query operations.  Our implementations of these
operations for segment trees worked by recursively descending through
the tree, either directly if the tree is stored as a recursive data
structure, or using simple operations on indices if the tree is stored
in an array. However, when storing the active nodes of a thinned tree
in a Fenwick array, it is not \emph{a priori} obvious what operations
on array indices will correspond to moving around the tree.  In order
to attack this problem, we first take a detour through a
domain-specific language for two's complement binary values.

\section{Two's Complement Binary} \label{sec:twos-complement}

The bit tricks usually employed to implement Fenwick trees rely on a
\emph{two's complement} representation of binary numbers, which allow
positive and negative numbers to be represented in a uniform way; for
example, a value consisting of all 1 bits represents $-1$.  We
therefore turn now to developing a domain-specific language, embedded
in Haskell, for manipulating two's complement binary representations.

First, we define a type of bits, with functions for inversion,
logical conjunction, and logical disjunction:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{4}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{Bit}\mathrel{=}\Conid{O}\mid \Conid{I}\;{}\<[19]%
\>[19]{}\mathbf{deriving}\;(\Conid{Eq},\Conid{Ord},\Conid{Show},\Conid{Enum}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\neg\mathbin{::}\Conid{Bit}\to \Conid{Bit}{}\<[E]%
\\
\>[B]{}\neg\mathrel{=}\lambda \mathbf{case}\;\{\mskip1.5mu \Conid{O}\to \Conid{I};\Conid{I}\to \Conid{O}\mskip1.5mu\}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}(\land),(\lor)\mathbin{::}\Conid{Bit}\to \Conid{Bit}\to \Conid{Bit}{}\<[E]%
\\
\>[B]{}\Conid{O}{}\<[4]%
\>[4]{}\land\anonymous {}\<[11]%
\>[11]{}\mathrel{=}\Conid{O}{}\<[E]%
\\
\>[B]{}\Conid{I}{}\<[4]%
\>[4]{}\land\Varid{b}{}\<[11]%
\>[11]{}\mathrel{=}\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Conid{I}{}\<[4]%
\>[4]{}\lor\anonymous {}\<[11]%
\>[11]{}\mathrel{=}\Conid{I}{}\<[E]%
\\
\>[B]{}\Conid{O}{}\<[4]%
\>[4]{}\lor\Varid{b}{}\<[11]%
\>[11]{}\mathrel{=}\Varid{b}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Next, we must define bit strings, \ie sequences of bits.  Rather than
fix a specific bit width, it will be much more elegant to work with
\emph{infinite} bit strings.\footnote{Some readers may recognize
  infinite two's complement bit strings as \term{$2$-adic} numbers, that
  is, $p$-adic numbers for the specific case $p = 2$, but nothing in
  our story depends on understanding the connection.} It is tempting
to use standard Haskell lists to represent potentially infinite bit
strings, but this leads to a number of problems. For example, equality
of infinite lists is not decidable, and there is no way in general to
convert from an infinite list of bits back to an \ensuremath{\Conid{Integer}}---how would
we know when to stop?  In fact, these practical problems stem from a
more fundamental one: infinite lists of bits are actually a bad
representation for two's complement bit strings, because of ``junk'',
that is, infinite lists of bits which do not correspond to values in
our intended semantic domain. For example, \ensuremath{\Varid{cycle}\;[\mskip1.5mu \Conid{I},\Conid{O}\mskip1.5mu]} is an
infinite list which alternates between \ensuremath{\Conid{I}} and \ensuremath{\Conid{O}} forever, but it
does not represent a valid two's complement encoding of an integer.
Even worse are non-periodic lists, such as the one with \ensuremath{\Conid{I}} at every
prime index and \ensuremath{\Conid{O}} everywhere else.

In fact, the bit strings we want are the \emph{eventually constant}
ones, that is, strings which eventually settle down to an infinite
tail of all zeros (which represent nonnegative integers) or all ones
(which represent negative integers).  Every such string has a finite
representation, so directly encoding eventually constant bit strings
in Haskell not only gets rid of the junk but also leads to elegant,
terminating algorithms for working with them.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{Bits}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{Rep}{}\<[9]%
\>[9]{}\mathbin{::}\Conid{Bit}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{Snoc}{}\<[9]%
\>[9]{}\mathbin{::}\mathbin{!}\Conid{Bits}\to \Conid{Bit}\to \Conid{Bits}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks


\ensuremath{\Conid{Rep}\;\Varid{b}} represents an infinite sequence of bit \ensuremath{\Varid{b}}, whereas \ensuremath{\Conid{Snoc}\;\Varid{bs}\;\Varid{b}} represents the bit string \ensuremath{\Varid{bs}} followed by a final bit \ensuremath{\Varid{b}}. We use
\ensuremath{\Conid{Snoc}}, rather than \ensuremath{\Conid{Cons}}, to match the way we usually write bit
strings, with the least significant bit last.  Note also the use of a
\term{strictness annotation} on the \ensuremath{\Conid{Bits}} field of \ensuremath{\Conid{Snoc}}; this is to
rule out infinite lists of bits using only \ensuremath{\Conid{Snoc}}, such as
\ensuremath{\Varid{bs}\mathrel{=}\Conid{Snoc}\;(\Conid{Snoc}\;\Varid{bs}\;\Conid{O})\;\Conid{I}}.  In other words, the only way to make a
non-bottom value of type \ensuremath{\Conid{Bits}} is to have a finite sequence of \ensuremath{\Conid{Snoc}}
finally terminated by \ensuremath{\Conid{Rep}}.

Although we have eliminated junk values, one remaining problem is that
there can be multiple distinct representations of the same value.  For
example, \ensuremath{\Conid{Snoc}\;(\Conid{Rep}\;\Conid{O})\;\Conid{O}} and \ensuremath{\Conid{Rep}\;\Conid{O}} both represent the infinite bit
string containing all zeros. However, we can solve this with a
carefully constructed \emph{bidirectional pattern synonym} \citep{pickering2016pattern}.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{toSnoc}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{toSnoc}\;(\Conid{Rep}\;\Varid{a})\mathrel{=}\Conid{Snoc}\;(\Conid{Rep}\;\Varid{a})\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{toSnoc}\;\Varid{as}\mathrel{=}\Varid{as}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{pattern}\;(\mathrel{:\!.})\mathbin{::}\Conid{Bits}\to \Conid{Bit}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{pattern}\;(\mathrel{:\!.})\;\Varid{bs}\;\Varid{b}\leftarrow (\Varid{toSnoc}\to \Conid{Snoc}\;\Varid{bs}\;\Varid{b}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{where}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Conid{Rep}\;\Varid{b}\mathrel{:\!.}\Varid{b'}\mid \Varid{b}\equiv \Varid{b'}\mathrel{=}\Conid{Rep}\;\Varid{b}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{bs}\mathrel{:\!.}\Varid{b}\mathrel{=}\Conid{Snoc}\;\Varid{bs}\;\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\texttt{\string{-\# COMPLETE (:.) \#-\string}}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Matching with the pattern \ensuremath{(\Varid{bs}\mathrel{:\!.}\Varid{b})} uses a \term{view pattern}
\citep{erwig2001pattern} to potentially expand a \ensuremath{\Conid{Rep}} one step into a
\ensuremath{\Conid{Snoc}}, so that we can pretend \ensuremath{\Conid{Bits}} values are always constructed
with \ensuremath{(\mathrel{:\!.})}.  Conversely, constructing a \ensuremath{\Conid{Bits}} with \ensuremath{(\mathrel{:\!.})} will do
nothing if we happen to snoc an identical bit \ensuremath{\Varid{b}} onto an existing
\ensuremath{\Conid{Rep}\;\Varid{b}}.  This ensures that as long as we stick to using \ensuremath{(\mathrel{:\!.})} and
never directly use \ensuremath{\Conid{Snoc}}, \ensuremath{\Conid{Bits}} values will always be
\emph{normalized} so that the terminal \ensuremath{\Conid{Rep}\;\Varid{b}} is immediately followed
by a different bit.  Finally, we mark the pattern \ensuremath{(\mathrel{:\!.})} as
\text{\ttfamily COMPLETE} on its own, since matching on \ensuremath{(\mathrel{:\!.})} is indeed
sufficient to handle every possible input of type \ensuremath{\Conid{Bits}}.  However, in
order to obtain terminating algorithms we will often include one or
more special cases for \ensuremath{\Conid{Rep}}.

Let's begin with some functions for converting \ensuremath{\Conid{Bits}} to and from
\ensuremath{\Conid{Integer}}, and for displaying \ensuremath{\Conid{Bits}} (intended only for testing).

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{4}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{toBits}\mathbin{::}\Conid{Int}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{toBits}\;\Varid{n}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{n}\equiv \mathrm{0}\mathrel{=}\Conid{Rep}\;\Conid{O}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{n}\equiv \mathbin{-}\mathrm{1}\mathrel{=}\Conid{Rep}\;\Conid{I}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{otherwise}{}\<[16]%
\>[16]{}\mathrel{=}\Varid{toBits}\;(\Varid{n}\mathbin{\Varid{`div`}}\mathrm{2})\mathrel{:\!.}\Varid{toEnum}\;(\Varid{n}\mathbin{\Varid{`mod`}}\mathrm{2}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{fromBits}\mathbin{::}\Conid{Bits}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{fromBits}\;(\Conid{Rep}\;\Conid{O})\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[B]{}\Varid{fromBits}\;(\Conid{Rep}\;\Conid{I})\mathrel{=}\mathbin{-}\mathrm{1}{}\<[E]%
\\
\>[B]{}\Varid{fromBits}\;(\Varid{bs}\mathrel{:\!.}\Varid{b})\mathrel{=}\mathrm{2}\cdot\Varid{fromBits}\;\Varid{bs}\mathbin{+}\Varid{fromEnum}\;\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{Show}\;\Conid{Bits}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{show}\mathrel{=}\Varid{reverse}\mathbin{\circ}\Varid{go}{}\<[E]%
\\
\>[3]{}\hsindent{1}{}\<[4]%
\>[4]{}\mathbf{where}{}\<[E]%
\\
\>[4]{}\hsindent{1}{}\<[5]%
\>[5]{}\Varid{go}\;(\Conid{Rep}\;\Varid{b})\mathrel{=}\Varid{replicate}\;\mathrm{3}\;(\Varid{showBit}\;\Varid{b})+\!+\text{\ttfamily \char34 ...\char34}{}\<[E]%
\\
\>[4]{}\hsindent{1}{}\<[5]%
\>[5]{}\Varid{go}\;(\Varid{bs}\mathrel{:\!.}\Varid{b})\mathrel{=}\Varid{showBit}\;\Varid{b}\mathbin{:}\Varid{go}\;\Varid{bs}{}\<[E]%
\\[\blanklineskip]%
\>[4]{}\hsindent{1}{}\<[5]%
\>[5]{}\Varid{showBit}\mathrel{=}(\text{\ttfamily \char34 01\char34}\mathbin{!!})\mathbin{\circ}\Varid{fromEnum}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
%$

Let's try it out, using QuickCheck \citep{claessen2000quickcheck} to
verify our conversion functions:

\begin{tabbing}\ttfamily
~ghci\char62{}~Rep~O~\char58{}\char46{}~O~\char58{}\char46{}~I~\char58{}\char46{}~O~\char58{}\char46{}~I\\
\ttfamily ~\char46{}\char46{}\char46{}000101\\
\ttfamily ~ghci\char62{}~Rep~I~\char58{}\char46{}~O~\char58{}\char46{}~I\\
\ttfamily ~\char46{}\char46{}\char46{}11101\\
\ttfamily ~ghci\char62{}~toBits~26\\
\ttfamily ~\char46{}\char46{}\char46{}00011010\\
\ttfamily ~ghci\char62{}~toBits~\char40{}\char45{}30\char41{}\\
\ttfamily ~\char46{}\char46{}\char46{}11100010\\
\ttfamily ~ghci\char62{}~fromBits~\char40{}toBits~\char40{}\char45{}30\char41{}\char41{}\\
\ttfamily ~\char45{}30\\
\ttfamily ~ghci\char62{}~quickCheck~\char36{}~\char92{}x~\char45{}\char62{}~fromBits~\char40{}toBits~x\char41{}~\char61{}\char61{}~x\\
\ttfamily ~\char43{}\char43{}\char43{}~OK\char44{}~passed~100~tests\char46{}
\end{tabbing}

We can now begin implementing some basic operations on \ensuremath{\Conid{Bits}}.  First,
incrementing and decrementing can be implemented recursively as
follows:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{inc}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{inc}\;(\Conid{Rep}\;\Conid{I}){}\<[16]%
\>[16]{}\mathrel{=}\Conid{Rep}\;\Conid{O}{}\<[E]%
\\
\>[B]{}\Varid{inc}\;(\Varid{bs}\mathrel{:\!.}\Conid{O}){}\<[16]%
\>[16]{}\mathrel{=}\Varid{bs}\mathrel{:\!.}\Conid{I}{}\<[E]%
\\
\>[B]{}\Varid{inc}\;(\Varid{bs}\mathrel{:\!.}\Conid{I}){}\<[16]%
\>[16]{}\mathrel{=}\Varid{inc}\;\Varid{bs}\mathrel{:\!.}\Conid{O}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{dec}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{dec}\;(\Conid{Rep}\;\Conid{O}){}\<[16]%
\>[16]{}\mathrel{=}\Conid{Rep}\;\Conid{I}{}\<[E]%
\\
\>[B]{}\Varid{dec}\;(\Varid{bs}\mathrel{:\!.}\Conid{I}){}\<[16]%
\>[16]{}\mathrel{=}\Varid{bs}\mathrel{:\!.}\Conid{O}{}\<[E]%
\\
\>[B]{}\Varid{dec}\;(\Varid{bs}\mathrel{:\!.}\Conid{O}){}\<[16]%
\>[16]{}\mathrel{=}\Varid{dec}\;\Varid{bs}\mathrel{:\!.}\Conid{I}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

The \emph{least significant bit}, or LSB, of a sequence of bits can be
defined as follows:
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{lsb}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{lsb}\;(\Conid{Rep}\;\Conid{O}){}\<[16]%
\>[16]{}\mathrel{=}\Conid{Rep}\;\Conid{O}{}\<[E]%
\\
\>[B]{}\Varid{lsb}\;(\Varid{bs}\mathrel{:\!.}\Conid{O}){}\<[16]%
\>[16]{}\mathrel{=}\Varid{lsb}\;\Varid{bs}\mathrel{:\!.}\Conid{O}{}\<[E]%
\\
\>[B]{}\Varid{lsb}\;(\anonymous \mathrel{:\!.}\Conid{I}){}\<[16]%
\>[16]{}\mathrel{=}\Conid{Rep}\;\Conid{O}\mathrel{:\!.}\Conid{I}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
Note that we add a special case for \ensuremath{\Conid{Rep}\;\Conid{O}} to ensure that \ensuremath{\Varid{lsb}} is
total. Technically, \ensuremath{\Conid{Rep}\;\Conid{O}} does not have a least significant bit, so
defining \ensuremath{\Varid{lsb}\;(\Conid{Rep}\;\Conid{O})\mathrel{=}\Conid{Rep}\;\Conid{O}} seems sensible.

\begin{tabbing}\ttfamily
~ghci\char62{}~toBits~26\\
\ttfamily ~\char34{}\char46{}\char46{}\char46{}00011010\char34{}\\
\ttfamily ~ghci\char62{}~lsb~\char36{}~toBits~26\\
\ttfamily ~\char34{}\char46{}\char46{}\char46{}00010\char34{}\\
\ttfamily ~ghci\char62{}~toBits~24\\
\ttfamily ~\char34{}\char46{}\char46{}\char46{}00011000\char34{}\\
\ttfamily ~ghci\char62{}~lsb~\char36{}~toBits~24\\
\ttfamily ~\char34{}\char46{}\char46{}\char46{}0001000\char34{}
\end{tabbing}

Bitwise logical conjunction can be defined straightforwardly.  Note
that we only need two cases; if the finite parts of the inputs have
different lengths, matching with \ensuremath{(\mathrel{:\!.})} will automatically expand the
shorter one to match the longer one.
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\owedge)\mathbin{::}\Conid{Bits}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Conid{Rep}\;\Varid{x}\owedge\Conid{Rep}\;\Varid{y}\mathrel{=}\Conid{Rep}\;(\Varid{x}\land\Varid{y}){}\<[E]%
\\
\>[B]{}(\Varid{xs}\mathrel{:\!.}\Varid{x})\owedge(\Varid{ys}\mathrel{:\!.}\Varid{y})\mathrel{=}(\Varid{xs}\owedge\Varid{ys})\mathrel{:\!.}(\Varid{x}\land\Varid{y}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
Bitwise inversion is likewise straightforward.
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{inv}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{inv}\;(\Conid{Rep}\;\Varid{b})\mathrel{=}\Conid{Rep}\;(\neg\;\Varid{b}){}\<[E]%
\\
\>[B]{}\Varid{inv}\;(\Varid{bs}\mathrel{:\!.}\Varid{b})\mathrel{=}\Varid{inv}\;\Varid{bs}\mathrel{:\!.}\neg\;\Varid{b}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
The above functions follow familiar patterns.  We could easily
generalize to eventually constant streams over an arbitrary element
type, and then implement \ensuremath{(\owedge)} in terms of a generic \ensuremath{\Varid{zipWith}} and \ensuremath{\Varid{inv}}
in terms of \ensuremath{\Varid{map}}.  However, for the present purpose we do not need
the extra generality.

We implement addition with the usual carry-propagation algorithm,
along with some special cases for \ensuremath{\Conid{Rep}}.
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\oplus)\mathbin{::}\Conid{Bits}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{xs}{}\<[12]%
\>[12]{}\oplus\Conid{Rep}\;\Conid{O}{}\<[27]%
\>[27]{}\mathrel{=}\Varid{xs}{}\<[E]%
\\
\>[B]{}\Conid{Rep}\;\Conid{O}{}\<[12]%
\>[12]{}\oplus\Varid{ys}{}\<[27]%
\>[27]{}\mathrel{=}\Varid{ys}{}\<[E]%
\\
\>[B]{}\Conid{Rep}\;\Conid{I}{}\<[12]%
\>[12]{}\oplus\Conid{Rep}\;\Conid{I}{}\<[27]%
\>[27]{}\mathrel{=}\Conid{Rep}\;\Conid{I}\mathrel{:\!.}\Conid{O}{}\<[E]%
\\
\>[B]{}\Conid{Snoc}\;\Varid{xs}\;\Conid{I}{}\<[12]%
\>[12]{}\oplus\Conid{Snoc}\;\Varid{ys}\;\Conid{I}{}\<[27]%
\>[27]{}\mathrel{=}\Varid{inc}\;(\Varid{xs}\oplus\Varid{ys})\mathrel{:\!.}\Conid{O}{}\<[E]%
\\
\>[B]{}\Conid{Snoc}\;\Varid{xs}\;\Varid{x}{}\<[12]%
\>[12]{}\oplus\Conid{Snoc}\;\Varid{ys}\;\Varid{y}{}\<[27]%
\>[27]{}\mathrel{=}(\Varid{xs}\oplus\Varid{ys})\mathrel{:\!.}(\Varid{x}\lor\Varid{y}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
It is not too hard to convince ourselves that this definition of
addition is terminating and yields correct results; but we can also be
fairly confident by just trying it with QuickCheck:

\begin{tabbing}\ttfamily
~ghci\char62{}~quickCheck~\char36{}~\char92{}x~y~\char45{}\char62{}~fromBits~\char40{}toBits~x~\char46{}\char43{}\char46{}~toBits~y\char41{}~\char61{}\char61{}~x~\char43{}~y\\
\ttfamily ~\char43{}\char43{}\char43{}~OK\char44{}~passed~100~tests\char46{}
\end{tabbing}

Finally, the following definition of negation is probably familiar to
anyone who has studied two's complement arithmetic; I leave it as an
exercise for the interested reader to prove that \ensuremath{\Varid{x}\oplus\Varid{neg}\;\Varid{x}\equiv \Conid{Rep}\;\Conid{O}} for all \ensuremath{\Varid{x}\mathbin{::}\Conid{Bits}}.
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{neg}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{neg}\mathrel{=}\Varid{inc}\mathbin{\circ}\Varid{inv}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

We now have the tools to resolve the first mystery of the Fenwick tree
implementation.
\begin{thm}
  For all \ensuremath{\Varid{x}\mathbin{::}\Conid{Bits}}, \[ \ensuremath{\Varid{lsb}\;\Varid{x}\mathrel{=}\Varid{x}\owedge\Varid{neg}\;\Varid{x}}. \]
\end{thm}
\begin{proof}
By induction on \ensuremath{\Varid{x}}.
\begin{itemize}
\item First, if \ensuremath{\Varid{x}\mathrel{=}\Conid{Rep}\;\Conid{O}}, it is an easy calculation to verify that
  \ensuremath{\Varid{lsb}\;\Varid{x}\mathrel{=}\Varid{x}\owedge\Varid{neg}\;\Varid{x}\mathrel{=}\Conid{Rep}\;\Conid{O}}.
\item Likewise, if \ensuremath{\Varid{x}\mathrel{=}\Conid{Rep}\;\Conid{I}}, both \ensuremath{\Varid{lsb}\;\Varid{x}} and \ensuremath{\Varid{x}\owedge\Varid{neg}\;\Varid{x}} reduce
  to \ensuremath{\Conid{Rep}\;\Conid{O}\mathrel{:\!.}\Conid{I}}.
\item If \ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{O}}, then \ensuremath{\Varid{lsb}\;\Varid{x}\mathrel{=}\Varid{lsb}\;(\Varid{xs}\mathrel{:\!.}\Conid{O})\mathrel{=}\Varid{lsb}\;\Varid{xs}\mathrel{:\!.}\Conid{O}}
  by definition, whereas
  \begin{sproof}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{O})\owedge\Varid{neg}\;(\Varid{xs}\mathrel{:\!.}\Conid{O})}}
    \reason{=}{Definition of \ensuremath{\Varid{neg}}}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{O})\owedge\Varid{inc}\;(\Varid{inv}\;(\Varid{xs}\mathrel{:\!.}\Conid{O}))}}
    \reason{=}{Definition of \ensuremath{\Varid{inv}} and \ensuremath{\neg}}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{O})\owedge\Varid{inc}\;(\Varid{inv}\;\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\Varid{inc}}}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{O})\owedge(\Varid{inc}\;(\Varid{inv}\;\Varid{xs})\mathrel{:\!.}\Conid{O})}}
    \reason{=}{Definition of \ensuremath{\owedge} and \ensuremath{\Varid{neg}}}
    \stmt{\ensuremath{(\Varid{xs}\owedge\Varid{neg}\;\Varid{xs})\mathrel{:\!.}\Conid{O}}}
    \reason{=}{Induction hypothesis}
    \stmt{\ensuremath{\Varid{lsb}\;\Varid{xs}\mathrel{:\!.}\Conid{O}}}
  \end{sproof}
\item Next, if \ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{I}}, then \ensuremath{\Varid{lsb}\;(\Varid{xs}\mathrel{:\!.}\Conid{I})\mathrel{=}\Conid{Rep}\;\Conid{O}\mathrel{:\!.}\Conid{I}} by
  definition, whereas
  \begin{sproof}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{I})\owedge\Varid{neg}\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\Varid{neg}}}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{I})\owedge\Varid{inc}\;(\Varid{inv}\;(\Varid{xs}\mathrel{:\!.}\Conid{I}))}}
    \reason{=}{Definition of \ensuremath{\Varid{inv}} and \ensuremath{\neg}}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{I})\owedge\Varid{inc}\;(\Varid{inv}\;\Varid{xs}\mathrel{:\!.}\Conid{O}))}}
    \reason{=}{Definition of \ensuremath{\Varid{inc}}}
    \stmt{\ensuremath{(\Varid{xs}\mathrel{:\!.}\Conid{I})\owedge(\Varid{inv}\;\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\owedge}}
    \stmt{\ensuremath{(\Varid{xs}\owedge\Varid{inv}\;\Varid{xs})\mathrel{:\!.}\Conid{I}}}
    \reason{=}{Bitwise AND of $xs$ and its inverse is \ensuremath{\Conid{Rep}\;\Conid{O}}}
    \stmt{\ensuremath{\Conid{Rep}\;\Conid{O}\mathrel{:\!.}\Conid{I}}}
  \end{sproof}
\end{itemize}
\vspace{-3\baselineskip}
\end{proof}

For the last equality we need a lemma that \ensuremath{\Varid{xs}\owedge\Varid{inv}\;\Varid{xs}\mathrel{=}\Conid{Rep}\;\Conid{O}}, which
should be intuitively clear and can easily be proved by induction as
well.

Finally, in order to express the index conversion functions we will
develop in the next section, we need a few more things in our DSL.
First, some functions to set and clear individual bits, and to test
whether particular bits are set:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{setTo}\mathbin{::}\Conid{Bit}\to \Conid{Int}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{setTo}\;\Varid{b'}\;\mathrm{0}\;(\Varid{bs}\mathrel{:\!.}\anonymous )\mathrel{=}\Varid{bs}\mathrel{:\!.}\Varid{b'}{}\<[E]%
\\
\>[B]{}\Varid{setTo}\;\Varid{b'}\;\Varid{k}\;(\Varid{bs}\mathrel{:\!.}\Varid{b})\mathrel{=}\Varid{setTo}\;\Varid{b'}\;(\Varid{k}\mathbin{-}\mathrm{1})\;\Varid{bs}\mathrel{:\!.}\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{set},\Varid{clear}\mathbin{::}\Conid{Int}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{set}\mathrel{=}\Varid{setTo}\;\Conid{I}{}\<[E]%
\\
\>[B]{}\Varid{clear}\mathrel{=}\Varid{setTo}\;\Conid{O}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{test}\mathbin{::}\Conid{Int}\to \Conid{Bits}\to \Conid{Bool}{}\<[E]%
\\
\>[B]{}\Varid{test}\;\mathrm{0}\;(\Varid{bs}\mathrel{:\!.}\Varid{b})\mathrel{=}\Varid{b}\equiv \Conid{I}{}\<[E]%
\\
\>[B]{}\Varid{test}\;\Varid{n}\;(\Varid{bs}\mathrel{:\!.}\anonymous )\mathrel{=}\Varid{test}\;(\Varid{n}\mathbin{-}\mathrm{1})\;\Varid{bs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{even},\Varid{odd}\mathbin{::}\Conid{Bits}\to \Conid{Bool}{}\<[E]%
\\
\>[B]{}\Varid{odd}\mathrel{=}\Varid{test}\;\mathrm{0}{}\<[E]%
\\
\>[B]{}\Varid{even}\mathrel{=}not\mathbin{\circ}\Varid{odd}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

The only other things we will need are left and right shift, and a
generic \ensuremath{\Varid{while}} combinator that iterates a given function, returning
the first iterate for which a predicate is false.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{shr}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{shr}\;(\Varid{bs}\mathrel{:\!.}\anonymous )\mathrel{=}\Varid{bs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{shl}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{shl}\mathrel{=}(\mathrel{:\!.}\Conid{O}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{while}\mathbin{::}(\Varid{a}\to \Conid{Bool})\to (\Varid{a}\to \Varid{a})\to \Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{while}\;\Varid{p}\;\Varid{f}\;\Varid{x}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{p}\;\Varid{x}{}\<[16]%
\>[16]{}\mathrel{=}\Varid{while}\;\Varid{p}\;\Varid{f}\;(\Varid{f}\;\Varid{x}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mid \Varid{otherwise}{}\<[16]%
\>[16]{}\mathrel{=}\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\section{Index Conversion} \label{sec:convert}

Before deriving our index conversion functions we must deal with one
slightly awkward fact.  In a traditional binary tree indexing scheme,
as shown in \pref{fig:bt-indexing}, the root has index $1$, every left
child is twice its parent, and every right child is one more than
twice its parent.  Recall that in a thinned segment tree, the root
node and every left child are active, with all right children being
inactive.  This makes the root an awkward special case---all active
nodes have an even index, \emph{except} the root, which has index $1$.
This makes it more difficult to check whether we are at an active
node---it is not enough to simply look at the least significant bit.

One easy way to fix this is simply to give the root index $2$, and
then proceed to label the rest of the nodes using the same
scheme---every left child is twice its parent, and every right child
is one more than twice its parent.  This results in the indexing shown
in \pref{fig:bt-indexing-two}, as if we had just taken the left
subtree of the tree rooted at $1$, and ignored the right subtree.  Of
course, this means about half the possible indices are omitted---but
that's not a problem, since we will only use these indices as an
intermediate step which will eventually get fused away.

\begin{figure}
  \centering
  \input{diagrams/Fenwick-diagrams-latex-fig11.pgf}
  \caption{Indexing a binary tree with $2$ at the root}
  \label{fig:bt-indexing-two}
\end{figure}

\pref{fig:bt-both} shows a binary tree where nodes have been numbered
in two different ways: the left side of each node shows the node's
binary tree index (with the root having index $2$).  The right side of
each node shows its index in the Fenwick array, if it has one (inactive
nodes simply have their right half greyed out).  The table underneath
shows the mapping from Fenwick array indices (top row) to binary tree
indices (bottom row).  As a larger example, \pref{fig:bt-both-big}
shows the same thing on a binary tree one level deeper.

\begin{figure}
  \centering

  \input{diagrams/Fenwick-diagrams-latex-fig12.pgf}

  \vspace{0.25in}

  \begin{tabular}{cccccccc}
    \textcolor{blue}{1} & \textcolor{blue}{2} & \textcolor{blue}{3}  & \textcolor{blue}{4} & \textcolor{blue}{5} & \textcolor{blue}{6} & \textcolor{blue}{7} & \textcolor{blue}{8} \\
    16 & 8 & 18 & 4 & 20 & 10 & 22 & 2
  \end{tabular}
  \caption{Binary tree labelled with both binary and Fenwick indexing} \label{fig:bt-both}
\end{figure}

\begin{figure}
  \centering
  \input{diagrams/Fenwick-diagrams-latex-fig13.pgf}

  \vspace{0.25in}

  \begin{tabular}{cccccccccccccccc}
  \textcolor{blue}{1} & \textcolor{blue}{2} & \textcolor{blue}{3} & \textcolor{blue}{4} & \textcolor{blue}{5} & \textcolor{blue}{6} & \textcolor{blue}{7} & \textcolor{blue}{8} & \textcolor{blue}{9} & \textcolor{blue}{10} & \textcolor{blue}{11} & \textcolor{blue}{12} & \textcolor{blue}{13} & \textcolor{blue}{14} & \textcolor{blue}{15} & \textcolor{blue}{16}
  \\
  32 & 16 & 34 & 8 & 36 & 18 & 38 & 4 & 40 & 20 & 42 & 10 & 44 & 22 & 46 & 2
  \end{tabular}
  \caption{Binary tree labelled with both binary and Fenwick indexing} \label{fig:bt-both-big}
\end{figure}

Our goal is to come up with a way to calculate the binary index for a
given Fenwick index or vice versa. Staring at the table in
\pref{fig:bt-both-big}, a few patterns stand out.  Of course, all the
numbers in the bottom row are even, which is precisely because the
binary tree is numbered in such a way that all active nodes have an
even index.  Second, we can see the even numbers $32, 34 \dots 46$, in
order, in all the odd positions.  These are exactly the leaves of the
tree, and indeed, every other node in the Fenwick array will be a leaf
from the original tree.  Alternating with these, in the even
positions, are the numbers $16\;\; 8\;\; 18\;\; 4 \dots$, which
correspond to all the non-leaf nodes; but these are exactly the
sequence of binary indices from the bottom row of the table in
\pref{fig:bt-both}---since the internal nodes in a tree of height 4
themselves constitute a tree of height 3, with the nodes occurring in
the same order.

These observations lead to the recurrence shown in \pref{fig:seqrec}
for the sequence $b_n$ of binary indices for the nodes stored in a
Fenwick array of length $2^n$: $b_0$ is just the singleton sequence
$[2]$, and otherwise $b_n$ is the even numbers
$2^{n+1}, 2^{n+1} + 2, \dots, 2^{n+1} + 2^n - 2$ interleaved with $b_{n-1}$.

\begin{figure}
\centering


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\interleaveop)\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}[\mskip1.5mu \mskip1.5mu]\interleaveop\anonymous \mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[B]{}(\Varid{x}\mathbin{:}\Varid{xs})\interleaveop\Varid{ys}\mathrel{=}\Varid{x}\mathbin{:}(\Varid{ys}\interleaveop\Varid{xs}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{b}\mathbin{::}\Conid{Int}\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{b}\;\mathrm{0}\mathrel{=}[\mskip1.5mu \mathrm{2}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{b}\;\Varid{n}\mathrel{=}\Varid{map}\;(\mathrm{2}\cdot)\;[\mskip1.5mu \mathrm{2}^ {\Varid{n}}\mathinner{\ldotp\ldotp}\mathrm{2}^ {\Varid{n}}\mathbin{+}\mathrm{2}^ {\Varid{n}\mathbin{-}\mathrm{1}}\mathbin{-}\mathrm{1}\mskip1.5mu]\interleaveop\Varid{b}\;(\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\caption{Recurrence for sequence of binary tree indices in a Fenwick
  array}
  \label{fig:seqrec}
\end{figure}

We can check that this does in fact reproduce the observed sequence
for $n = 4$:

\begin{tabbing}\ttfamily
~ghci\char62{}~b~4\\
\ttfamily ~\char91{}32\char44{}16\char44{}34\char44{}8\char44{}36\char44{}18\char44{}38\char44{}4\char44{}40\char44{}20\char44{}42\char44{}10\char44{}44\char44{}22\char44{}46\char44{}2\char93{}
\end{tabbing}

Let \ensuremath{\Varid{s}\mathbin{!}\Varid{k}} denote the $k$th item in the list $s$ (counting from 1),
as defined in \pref{fig:index-interleave}.  The same figure also lists
two easy lemmas about the interaction between indexing and
interleaving, namely, \ensuremath{(\Varid{xs}\interleaveop\Varid{ys})\mathbin{!}(\mathrm{2}\cdot\Varid{j})\mathrel{=}\Varid{ys}\mathbin{!}\Varid{j}} and
\ensuremath{(\Varid{xs}\interleaveop\Varid{ys})\mathbin{!}(\mathrm{2}\cdot\Varid{j}\mathbin{-}\mathrm{1})\mathrel{=}\Varid{xs}\mathbin{!}\Varid{j}} (as long as \ensuremath{\Varid{xs}} and \ensuremath{\Varid{ys}}
have equal lengths).  With these in hand, we can
define the Fenwick to binary index conversion function as
\[ \ensuremath{\Varid{f2b}\;\Varid{n}\;\Varid{k}\mathrel{=}\Varid{b}\;\Varid{n}\mathbin{!}\Varid{k}}. \]
Of course, since $b_n$ is of length $2^n$, this function is only
defined on the range $[1, 2^n]$.

\begin{figure}
  \centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\Varid{a}\mathbin{:}\anonymous )\mathbin{!}\mathrm{1}\mathrel{=}\Varid{a}{}\<[E]%
\\
\>[B]{}(\anonymous \mathbin{:}\Varid{as})\mathbin{!}\Varid{k}\mathrel{=}\Varid{as}\mathbin{!}(\Varid{k}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\onelinecomment  If \ensuremath{|\Varid{xs}|\equiv |\Varid{ys}|}:}{}\<[E]%
\\
\>[B]{}(\Varid{xs}\interleaveop\Varid{ys})\mathbin{!}(\mathrm{2}\cdot\Varid{j}){}\<[35]%
\>[35]{}\mathrel{=}\Varid{ys}\mathbin{!}\Varid{j}{}\<[E]%
\\
\>[B]{}(\Varid{xs}\interleaveop\Varid{ys})\mathbin{!}(\mathrm{2}\cdot\Varid{j}\mathbin{-}\mathrm{1}){}\<[35]%
\>[35]{}\mathrel{=}\Varid{xs}\mathbin{!}\Varid{j}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

  \caption{Indexing and interleaving}
  \label{fig:index-interleave}
\end{figure}

We can now simplify the  definition of \ensuremath{\Varid{f2b}} as follows. First of all, for even
inputs, we have

\begin{sproof}
  \stmt{\ensuremath{\Varid{f2b}\;\Varid{n}\;(\mathrm{2}\cdot\Varid{j})}}
  \reason{=}{Definition of \ensuremath{\Varid{f2b}}}
  \stmt{\ensuremath{\Varid{b}\;\Varid{n}\mathbin{!}(\mathrm{2}\cdot\Varid{j})}}
  \reason{=}{Definition of \ensuremath{\Varid{b}}}
  \stmt{\ensuremath{(\Varid{map}\;(\mathrm{2}\cdot)\;[\mskip1.5mu \mathrm{2}^ {\Varid{n}}\mathinner{\ldotp\ldotp}\mathrm{2}^ {\Varid{n}}\mathbin{+}\mathrm{2}^ {\Varid{n}\mathbin{-}\mathrm{1}}\mathbin{-}\mathrm{1}\mskip1.5mu]\interleaveop\Varid{b}\;(\Varid{n}\mathbin{-}\mathrm{1}))\mathbin{!}(\mathrm{2}\cdot\Varid{j})}}
  \reason{=}{\ensuremath{\interleaveop\mathbin{-!}} lemma}
  \stmt{b (n-1) ! j}
  \reason{=}{Definition of \ensuremath{\Varid{f2b}}}
  \stmt{\ensuremath{\Varid{f2b}\;(\Varid{n}\mathbin{-}\mathrm{1})\;\Varid{j}}.}
\end{sproof}
Whereas for odd inputs,
\begin{sproof}
  \stmt{\ensuremath{\Varid{f2b}\;\Varid{n}\;(\mathrm{2}\cdot\Varid{j}\mathbin{-}\mathrm{1})}}
  \reason{=}{Definition of \ensuremath{\Varid{f2b}}}
  \stmt{\ensuremath{\Varid{b}\;\Varid{n}\mathbin{!}(\mathrm{2}\cdot\Varid{j}\mathbin{-}\mathrm{1})}}
  \reason{=}{Definition of \ensuremath{\Varid{b}}}
  \stmt{\ensuremath{(\Varid{map}\;(\mathrm{2}\cdot)\;[\mskip1.5mu \mathrm{2}^ {\Varid{n}}\mathinner{\ldotp\ldotp}\mathrm{2}^ {\Varid{n}}\mathbin{+}\mathrm{2}^ {\Varid{n}\mathbin{-}\mathrm{1}}\mathbin{-}\mathrm{1}\mskip1.5mu]\interleaveop\Varid{b}\;(\Varid{n}\mathbin{-}\mathrm{1}))\mathbin{!}(\mathrm{2}\cdot\Varid{j}\mathbin{-}\mathrm{1})}}
  \reason{=}{\ensuremath{\interleaveop\mathbin{-!}} lemma}
  \stmt{\ensuremath{\Varid{map}\;(\mathrm{2}\cdot)\;[\mskip1.5mu \mathrm{2}^ {\Varid{n}}\mathinner{\ldotp\ldotp}\mathrm{2}^ {\Varid{n}}\mathbin{+}\mathrm{2}^ {\Varid{n}\mathbin{-}\mathrm{1}}\mathbin{-}\mathrm{1}\mskip1.5mu]\mathbin{!}\Varid{j}}}
  \reason{=}{Definition of \ensuremath{\Varid{map}}, algebra}
  \stmt{\ensuremath{\mathrm{2}\cdot(\mathrm{2}^ {\Varid{n}}\mathbin{+}\Varid{j}\mathbin{-}\mathrm{1})}}
  \reason{=}{algebra}
  \stmt{\ensuremath{\mathrm{2}^ {\Varid{n}\mathbin{+}\mathrm{1}}\mathbin{+}\mathrm{2}\;\Varid{j}\mathbin{-}\mathrm{2}}}
\end{sproof}
Thus we have
\[ \ensuremath{\Varid{f2b}\;\Varid{n}\;\Varid{k}} = \begin{cases} \ensuremath{\Varid{f2b}\;(\Varid{n}\mathbin{-}\mathrm{1})\;(\Varid{k}\mathbin{/}\mathrm{2})} & k \text{ even} \\ 2^{n+1}
    + k - 1 & k \text{ odd} \end{cases} \] Note that when $n = 0$ we
must have $k = 1$, and hence $\ensuremath{\Varid{f2b}\;\mathrm{0}\;\mathrm{1}} = 2^0 + 1 - 1 = 1$, as
required, so this definition is valid for all $n \geq 0$.  Now factor
$k$ uniquely as $2^a \cdot b$ where $b$ is odd.  Then by induction we
can see that
\[ \ensuremath{\Varid{f2b}\;\Varid{n}\;(\mathrm{2}^ {\Varid{a}}\cdot\Varid{b})\mathrel{=}\Varid{f2b}\;(\Varid{n}\mathbin{-}\Varid{a})\;\Varid{b}} = 2^{n-a+1} + b - 1. \] So,
in other words, computing \ensuremath{\Varid{f2b}} consists of repeatedly dividing by 2
(\ie right bit shifts) as long as the input is even, and then finally
decrementing and adding a power of $2$.  However, knowing what power
of $2$ to add at the end depends on knowing how many times we shifted.
A better way to think of it is to add $2^{n+1}$ at the
\emph{beginning}, and then let it be shifted along with everything
else.  Thus, we have the following definition of \ensuremath{\Varid{f2b'}} using our
\ensuremath{\Conid{Bits}} DSL.  Defining \ensuremath{\Varid{shift}\;\Varid{n}\mathrel{=}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{set}\;\Varid{n}} separately
will make some of our proofs more compact later.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{shift}\mathbin{::}\Conid{Int}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{shift}\;\Varid{n}\mathrel{=}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{set}\;\Varid{n}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{f2b'}\mathbin{::}\Conid{Int}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{f2b'}\;\Varid{n}\mathrel{=}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks


For example, we can verify that this produces identical results to
\ensuremath{\Varid{f2b}\;\mathrm{4}} on the range $[1, 2^4]$ (for convenience, we define \ensuremath{(\Varid{f}\mathbin{===}\Varid{g})\;\Varid{k}\mathrel{=}\Varid{f}\;\Varid{k}\equiv \Varid{g}\;\Varid{k}}):
\begin{Verbatim}
ghci> all (f2b 4 === fromBits . f2b' 4 . toBits) [1 .. 2^4]
True
\end{Verbatim}

We now turn to deriving \ensuremath{\Varid{b2f}\;\Varid{n}}, which converts back from binary to
Fenwick indices. \ensuremath{\Varid{b2f}\;\Varid{n}} should be a left inverse to \ensuremath{\Varid{f2b}\;\Varid{n}}, that is,
for any $k \in [1, 2^n]$ we should have \ensuremath{\Varid{b2f}\;\Varid{n}\;(\Varid{f2b}\;\Varid{n}\;\Varid{k})\equiv \Varid{k}}. If $k$
is an input to \ensuremath{\Varid{f2b}}, we have $k = 2^a \cdot b \leq 2^n$, and so
$b-1 < 2^{n-a}$.  Hence, given the output
$\ensuremath{\Varid{f2b}\;\Varid{n}\;\Varid{k}} = m = 2^{n-a+1} + b - 1$, the highest bit of $m$ is
$2^{n-a+1}$, and the rest of the bits represent $b-1$.  So, in
general, given some $m$ which is the output of \ensuremath{\Varid{f2b}\;\Varid{n}}, we can write
it uniquely as $m = 2^c + d$ where $d < 2^{c-1}$; then
\[ \ensuremath{\Varid{b2f}\;\Varid{n}\;(\mathrm{2}^ {\Varid{c}}\mathbin{+}\Varid{d})\mathrel{=}\mathrm{2}^ {\Varid{n}\mathbin{-}\Varid{c}\mathbin{+}\mathrm{1}}\cdot(\Varid{d}\mathbin{+}\mathrm{1})}. \] In other words,
given the input $2^c + d$, we subtract off the highest bit $2^c$,
increment, then left shift $n-c+1$ times.  Again, though, there is a
simpler way: we can increment first (note since $d < 2^{c-1}$,
incrementing cannot disturb the bit at $2^c$), then left shift enough
times to bring the leftmost bit into position $n+1$, and finally
remove it.  That is:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{unshift}\mathbin{::}\Conid{Int}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{unshift}\;\Varid{n}\mathrel{=}\Varid{clear}\;\Varid{n}\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;\Varid{n})\;\Varid{shl}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{b2f'}\mathbin{::}\Conid{Int}\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{b2f'}\;\Varid{n}\mathrel{=}\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{inc}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Verifying:
\begin{tabbing}\ttfamily
~ghci\char62{}~all~\char40{}fromBits~\char46{}~b2f\char39{}~4~\char46{}~f2b\char39{}~4~\char46{}~toBits~\char61{}\char61{}\char61{}~id\char41{}~\char91{}1~\char46{}\char46{}~2\char94{}4\char93{}\\
\ttfamily ~True
\end{tabbing}

\section{Deriving Fenwick Operations} \label{sec:fenwick-ops}

We can now finally derive the required operations on Fenwick array
indices for moving through the tree, by starting with operations on a
binary indexed tree and conjugating by conversion to and from Fenwick
indices.  First, in order to fuse away the resulting conversion, we
will need a few lemmas.

\begin{lem}[shr-inc-dec] \label{lem:incshr}
  For all \ensuremath{\Varid{bs}\mathbin{::}\Conid{Bits}} which are \ensuremath{\Varid{odd}} (that is, end with \ensuremath{\Conid{I}}),
  \begin{itemize}
  \item \ensuremath{(\Varid{shr}\mathbin{\circ}\Varid{dec})\;\Varid{bs}\mathrel{=}\Varid{shr}\;\Varid{bs}}
  \item \ensuremath{(\Varid{shr}\mathbin{\circ}\Varid{inc})\;\Varid{bs}\mathrel{=}(\Varid{inc}\mathbin{\circ}\Varid{shr})\;\Varid{bs}}
  \end{itemize}
\end{lem}
\begin{proof}
  Both are immediate by definition.
\end{proof}

\begin{lem}[while-inc-dec] \label{lem:incwhile}
  The following both hold for all \ensuremath{\Conid{Bits}} values:
  \begin{itemize}
  \item \ensuremath{\Varid{inc}\mathbin{\circ}\Varid{while}\;\Varid{odd}\;\Varid{shr}\mathrel{=}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{inc}}
  \item \ensuremath{\Varid{dec}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathrel{=}\Varid{while}\;\Varid{odd}\;\Varid{shr}\mathbin{\circ}\Varid{dec}}
  \end{itemize}
\end{lem}
\begin{proof}
  Easy proof by induction on \ensuremath{\Conid{Bits}}.  For example, for the \ensuremath{\Varid{inc}} case,
  the functions on both sides discard consecutive 1 bits and then flip
  the first 0 bit to a 1.
\end{proof}

Finally, we will need a lemma about shifting zero bits in and out of
the right side of a value.

\begin{lem}[shl-shr] \label{lem:shlshr}
  For all $0 < x < 2^{n+2}$,
  \[ \ensuremath{(\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr})\;\Varid{x}\mathrel{=}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}\;\Varid{x}}. \]
\end{lem}
\begin{proof}
  Intuitively, this says that if we first shift out all the zero bits
  and then left shift until bit $n+1$ is set, we could get the same
  result by forgetting about the right shifts entirely; shifting out
  zero bits and then shifting them back in should be the identity.

  Formally, the proof is by induction on \ensuremath{\Varid{x}}.  If \ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{I}} is odd, the equality is
  immediate since \ensuremath{\Varid{while}\;\Varid{even}\;\Varid{shr}\;\Varid{x}\mathrel{=}\Varid{x}}. Otherwise, if \ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{O}},
  on the left-hand side the \ensuremath{\Conid{O}} is immediately discarded by \ensuremath{\Varid{shr}},
  whereas on the right-hand side \ensuremath{\Varid{xs}\mathrel{:\!.}\Conid{O}\mathrel{=}\Varid{shl}\;\Varid{xs}}, and the extra
  \ensuremath{\Varid{shl}} can be absorbed into the \ensuremath{\Varid{while}} since $\ensuremath{\Varid{xs}} < 2^{n+1}$.  What
  remains is simply the induction hypothesis.
\end{proof}

With these lemmas under our belt, let's see how to move around a
Fenwick array in order to implement \ensuremath{\Varid{update}} and \ensuremath{\Varid{query}}; we'll begin
with \ensuremath{\Varid{update}}. When implementing the \ensuremath{\Varid{update}} operation, we need to start at a leaf
and follow the path up to the root, updating all the active nodes
along the way.  In fact, for any given leaf, its closest active parent
is precisely the node stored in the slot that used to correspond to
that leaf (see \pref{fig:right-leaning}).  So to update index $i$, we
just need to start at index $i$ in the Fenwick array, and then
repeatedly find the closest active parent, updating as we go.  Recall
that the imperative code for \ensuremath{\Varid{update}} works this way, apparently
finding the closest active parent at each step by adding the LSB of
the current index:
\inputminted[fontsize=\footnotesize,firstline=8,lastline=10]{java}{FenwickTree.java}
\noindent
Let's see how to derive this behavior.

To find the closest active parent of a node under a binary indexing
scheme, we first move up to the immediate parent (by dividing the
index by two, \ie performing a right bit shift); then continue moving
up to the next immediate parent as long as the current node is a right
child (\ie has an odd index).  This yields the definition:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{activeParentBinary}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{activeParentBinary}\mathrel{=}\Varid{while}\;\Varid{odd}\;\Varid{shr}\mathbin{\circ}\Varid{shr}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

This is why we used the slightly strange indexing scheme with the root
having index $2$---otherwise this definition would not work for any
node whose active parent is the root!

Now, to derive the corresponding operation on Fenwick indices, we
conjugate by conversion to and from Fenwick indices, and compute as
follows.  To make the computation easier to read, the portion being
rewritten is underlined at each step.

\begin{sproof}
  \stmt{\ensuremath{\Varid{b2f'}\;\Varid{n}\mathbin{\circ}\Varid{activeParentBinary}\mathbin{\circ}\Varid{f2b'}\;\Varid{n}}}
  \reason{=}{expand definitions}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{inc}\mathbin{\circ}\Varid{while}\;\Varid{odd}\;\Varid{shr}}\mathbin{\circ}\Varid{shr}\mathbin{\circ}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\pref{lem:incwhile} (while-inc-dec)}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{inc}\mathbin{\circ}\underline{\Varid{shr}\mathbin{\circ}\Varid{dec}}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\pref{lem:incshr} (shr-inc-dec); \ensuremath{\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})\;\Varid{x}} is always odd}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\underline{\Varid{inc}\mathbin{\circ}\Varid{shr}}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\pref{lem:incshr} (shr-inc-dec)}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{shr}}\mathbin{\circ}\Varid{inc}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\ensuremath{\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{shr}\mathrel{=}\Varid{while}\;\Varid{even}\;\Varid{shr}} on an even input}
  \stmt{\ensuremath{\underline{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{inc}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{Definition of \ensuremath{\Varid{unshift}}}
  \stmt{\ensuremath{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}}\mathbin{\circ}\Varid{inc}\mathbin{\circ}\underline{\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}}
  \reason{=}{\pref{lem:shlshr} (shl-shr); definition of \ensuremath{\Varid{shift}}}
  \stmt{\ensuremath{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}\mathbin{\circ}\Varid{inc}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
\end{sproof}
In the final step, since the input $x$ satisfies $x \leq 2^n$, we have
$\ensuremath{\Varid{inc}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})} < 2^{n+2}$, so \pref{lem:shlshr} applies.

Reading from right to left, the pipeline we have just computed
performs the following steps:
\begin{enumerate}
\item Set bit $n+1$
\item Shift out consecutive zeros until finding the least significant
  $1$ bit
\item Increment
\item Shift zeros back in to bring the most significant bit back to position $n+1$,
  then clear it.
\end{enumerate}

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig14.pgf}
\end{center}
\caption{Adding LSB with a sentinel bit + shifts} \label{fig:bitspipeline}
\end{figure}

Intuitively, this does look a lot like adding the LSB!  In general, to
find the LSB, one must shift through consecutive $0$ bits until
finding the first $1$; the question is how to keep track of how many
$0$ bits were shifted on the way.  The \ensuremath{\Varid{lsb}} function itself keeps
track via the recursion stack; after finding the first $1$ bit, the
recursion stack unwinds and re-snocs all the $0$ bits recursed through
on the way.  The above pipeline represents an alternative approach:
set bit $n+1$ as a ``sentinel'' to keep track of how much we have
shifted; right shift until the first $1$ is literally in the ones
place, at which point we increment; and then shift all the $0$ bits
back in by doing left shifts until the sentinel bit gets back to
the $n+1$ place. One example of this process is illustrated in
\pref{fig:bitspipeline}. Of course, this only works for values that are
sufficiently small that the sentinel bit will not be disturbed
throughout the operation.

To make this more formal, we begin by defining a helper function
\ensuremath{\Varid{atLSB}}, which does an operation ``at the LSB'', that is, it shifts
out 0 bits until finding a 1, applies the given function, then
restores the 0 bits.
\newpage
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{atLSB}\mathbin{::}(\Conid{Bits}\to \Conid{Bits})\to \Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{atLSB}\;\anonymous \;(\Conid{Rep}\;\Conid{O})\mathrel{=}\Conid{Rep}\;\Conid{O}{}\<[E]%
\\
\>[B]{}\Varid{atLSB}\;\Varid{f}\;(\Varid{bs}\mathrel{:\!.}\Conid{O})\mathrel{=}\Varid{atLSB}\;\Varid{f}\;\Varid{bs}\mathrel{:\!.}\Conid{O}{}\<[E]%
\\
\>[B]{}\Varid{atLSB}\;\Varid{f}\;\Varid{bs}\mathrel{=}\Varid{f}\;\Varid{bs}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\begin{lem}[add-lsb] \label{lem:addlsb}
  For all \ensuremath{\Varid{x}\mathbin{::}\Conid{Bits}}, \ensuremath{\Varid{x}\mathbin{+}\Varid{lsb}\;\Varid{x}\mathrel{=}\Varid{atLSB}\;\Varid{inc}\;\Varid{x}} and \ensuremath{\Varid{x}\mathbin{-}\Varid{lsb}\;\Varid{x}\mathrel{=}\Varid{atLSB}\;\Varid{dec}\;\Varid{x}}.
\end{lem}

\begin{proof}
  Straightforward induction on $x$.
\end{proof}

We can formally relate the ``shifting with a sentinel'' scheme to
the use of \ensuremath{\Varid{atLSB}}, with the following (admittedly rather technical)
lemma:

\begin{restatable}[sentinel]{lem}{sentinel} \label{lem:sentinel-scheme} Let $n \geq 1$ and let \ensuremath{\Varid{f}\mathbin{::}\Conid{Bits}\to \Conid{Bits}} be a function such that
  \begin{enumerate}
  \item \ensuremath{(\Varid{f}\mathbin{\circ}\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{x}\mathrel{=}(\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f})\;\Varid{x}} for any $0 < x < 2^n$, and
  \item $\ensuremath{\Varid{f}\;\Varid{x}} < 2^{n+1}$ for any $0 < x < 2^n + 2^{n-1}$.
  \end{enumerate}
  Then for all $0 < x < 2^n$,
  \[ \ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{x}\mathrel{=}\Varid{atLSB}\;\Varid{f}\;\Varid{x}}. \]
\end{restatable}

The proof is rather tedious and not all that illuminating, so we omit
it
\ifJFP
(an extended version including a full proof may be found on the
author's website, at \url{http://ozark.hendrix.edu/~yorgey/pub/Fenwick-ext.pdf}).
\else
here (a detailed proof can be found in an appendix).
\fi
However, we do note that both \ensuremath{\Varid{inc}} and
\ensuremath{\Varid{dec}} fit the criteria for \ensuremath{\Varid{f}}: incrementing or decrementing some
$0 < x < 2^n$ cannot affect the $(n+1)$st bit as long as $n \geq 1$,
and the result of incrementing or decrementing a number less than
$2^n + 2^{n-1}$ will be a number less than $2^{n+1}$.  We can
now put all the pieces together show that adding the LSB at each step
is the correct way to implement \ensuremath{\Varid{update}}.

\begin{thm}
  Adding the LSB is the correct way to move up a Fenwick-indexed tree
  to the nearest active parent, that is,
  \[ \ensuremath{\Varid{activeParentFenwick}\mathrel{=}\Varid{b2f'}\;\Varid{n}\mathbin{\circ}\Varid{activeParentBinary}\mathbin{\circ}\Varid{f2b'}\;\Varid{n}\mathrel{=}\lambda \Varid{x}\to \Varid{x}\mathbin{+}\Varid{lsb}\;\Varid{x}} \] everywhere on the range $[1, 2^n)$. (We exclude
  $2^n$ since it corresponds to the root of the tree under a Fenwick
  indexing scheme.)
\end{thm}
\begin{proof}
\begin{sproof}
  \stmt{\ensuremath{\Varid{b2f'}\;\Varid{n}\mathbin{\circ}\Varid{activeParentBinary}\mathbin{\circ}\Varid{f2b'}\;\Varid{n}}}
  \reason{=}{Previous calculation}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{inc}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\pref{lem:sentinel-scheme} (sentinel)}
  \stmt{\ensuremath{\Varid{atLSB}\;\Varid{inc}}}
  \reason{=}{\pref{lem:addlsb} (add-lsb)}
  \stmt{\ensuremath{\lambda \Varid{x}\to \Varid{x}\mathbin{+}\Varid{lsb}\;\Varid{x}}}
\end{sproof}
\vspace{-3\baselineskip}
\end{proof}

We can carry out a similar process to derive an implementation for
prefix query (which suppsedly involves \emph{subtracting} the LSB).
Again, if we want to compute the sum of $[1, j]$, we can start at
index $j$ in the Fenwick array, which stores the sum of the unique
segment ending at $j$.  If the node at index $j$ stores the segment
$[i,j]$, we next need to find the unique node storing a segment that
ends at $i-1$.  We can do this repeatedly, adding up segments as we
go.

\begin{figure}
\begin{center}
\input{diagrams/Fenwick-diagrams-latex-fig15.pgf}
\end{center}
\caption{Moving up a segment tree to find successive prefix
  segments} \label{fig:segment-tree-prefix-query-up}
\end{figure}

Staring at \pref{fig:segment-tree-prefix-query-up} for inspiration, we
can see that what we want to do is find the \emph{left sibling} of our
\emph{closest inactive parent}, that is, we go up until finding the
first ancestor which is a right child, then go to its left sibling.
Under a binary indexing scheme, this can be implemented simply as:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{prevSegmentBinary}\mathbin{::}\Conid{Bits}\to \Conid{Bits}{}\<[E]%
\\
\>[B]{}\Varid{prevSegmentBinary}\mathrel{=}\Varid{dec}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\begin{thm}
  Subtracting the LSB is the correct way to move up a Fenwick-indexed
  tree to the to active node covering the segment previous to the
  current one, that is,
  \[ \ensuremath{\Varid{prevSegmentFenwick}\mathrel{=}\Varid{b2f'}\;\Varid{n}\mathbin{\circ}\Varid{prevSegmentBinary}\mathbin{\circ}\Varid{f2b'}\;\Varid{n}\mathrel{=}\lambda \Varid{x}\to \Varid{x}\mathbin{-}\Varid{lsb}\;\Varid{x}} \]
  everywhere on the range $[1, 2^n)$.
\end{thm}
\begin{proof}
\begin{sproof}
  \stmt{\ensuremath{\Varid{b2f'}\;\Varid{n}\mathbin{\circ}\Varid{prevSegmentBinary}\mathbin{\circ}\Varid{f2b'}\;\Varid{n}}}
  \reason{=}{expand definitions}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{inc}\mathbin{\circ}\Varid{dec}}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\ensuremath{\Varid{inc}\mathbin{\circ}\Varid{dec}\mathrel{=}\Varid{id}}}
  \stmt{\ensuremath{\underline{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{Definition of \ensuremath{\Varid{unshift}}}
  \stmt{\ensuremath{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}}\mathbin{\circ}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\pref{lem:shlshr} (shl-shr)}
  \stmt{\ensuremath{\underline{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}}\mathbin{\circ}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{Definition of \ensuremath{\Varid{unshift}}}
  \stmt{\ensuremath{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{dec}\mathbin{\circ}\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
  \reason{=}{\pref{lem:sentinel-scheme} (sentinel)}
  \stmt{\ensuremath{\Varid{atLSB}\;\Varid{dec}}}
  \reason{=}{\pref{lem:addlsb} (add-lsb)}
  \stmt{\ensuremath{\lambda \Varid{x}\to \Varid{x}\mathbin{-}\Varid{lsb}\;\Varid{x}}}
\end{sproof}
\vspace{-3\baselineskip}
\end{proof}

\section{Conclusion}

Historically, to my knowledge, Fenwick trees were not actually
developed as an optimization of segment trees as presented here.  This
has merely been a fictional---but hopefully illuminating---alternate
history of ideas, highlighting the power of functional thinking,
domain-specific languages, and equational reasoning to explore
relationships between different structures and algorithms.  As future
work, it would be interesting to explore some of the mentioned
generalizations of segment trees, to see whether one can derive
Fenwick-like structures that support additional operations.

\section*{Acknowledgements}

Thanks to the anonymous JFP reviewers for their helpful feedback,
which resulted in a much improved presentation.  Thanks also to Penn PL
Club for the opportunity to present an early version of this work.
\bigskip

\noindent \textbf{Conflicts of Interest}. None

\bibliographystyle{JFPlike}
\bibliography{fenwick}

\ifJFP
\else
\section*{Appendix}

For completeness, we include here a proof of
\pref{lem:sentinel-scheme}, which shows that for functions $f$
satisfying suitable conditions, \ensuremath{\Varid{atLSB}\;\Varid{f}} has the same effect as the
``sentinel scheme'' where we set a sentinel bit, shift to bring the
LSB to the ones place, perform $f$, then shift back.  To complete the
proof of this lemma we first need a few more.

\begin{lem} \label{lem:clearshl}
  \ensuremath{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{shl}\mathrel{=}\Varid{shl}\mathbin{\circ}\Varid{clear}\;\Varid{n}}
\end{lem}
\begin{proof}
  Immediate by a simple computation.
\end{proof}

\begin{lem} \label{lem:shlwhile}
 For all $n \geq 0$ and $0 < x < 2^{n+1}$, \[ \ensuremath{(\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl})\;\Varid{x}\mathrel{=}(\Varid{shl}\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;\Varid{n})\;\Varid{shl})\;\Varid{x}}. \]
\end{lem}
\begin{proof}
  Intuitively, if $x$ is small enough, we can either keep shifting
  left until bit $n+1$ is set, or we can shift left until bit $n$ is
  set and then shift left one additional time.

  Formally, the proof is by induction on the size of $2^{n+1} - x$.
  First, if $2^n \leq x < 2^{n+1}$, then bit $n$ of $x$ must be $1$, and both
  sides will be equal to \ensuremath{\Varid{shl}\;\Varid{x}}.  Otherwise, suppose $x < 2^n$.  Then
  \begin{sproof}
    \stmt{\ensuremath{(\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl})\;\Varid{x}}}
    \reason{=}{$x < 2^{n+1}$}
    \stmt{\ensuremath{(\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl})\;(\Varid{shl}\;\Varid{x})}}
    \reason{=}{Induction hypothesis, since $2^{n+1} - \ensuremath{\Varid{shl}\;\Varid{x}} < 2^{n+1} - x$}
    \stmt{\ensuremath{(\Varid{shl}\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;\Varid{n})\;\Varid{shl})\;(\Varid{shl}\;\Varid{x})}}
    \reason{=}{$x < 2^n$}
    \stmt{\ensuremath{(\Varid{shl}\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;\Varid{n})\;\Varid{shl})\;\Varid{x}}}
  \end{sproof}
\vspace{-3\baselineskip}
\end{proof}

Now we can finally prove \pref{lem:sentinel-scheme}, which we restate
here for convenience.

\sentinel*

\begin{proof}
  By induction on $x$.  First, suppose \ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{I}}.  In that case,
  \ensuremath{\Varid{atLSB}\;\Varid{f}\;(\Varid{xs}\mathrel{:\!.}\Conid{I})\mathrel{=}\Varid{f}\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}, and
  \begin{sproof}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\underline{\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})})\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\Varid{shift}}}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\underline{\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})})\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\Varid{set}}}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\underline{\Varid{while}\;\Varid{even}\;\Varid{shr}})\;(\Varid{set}\;\Varid{n}\;\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{\ensuremath{\Varid{while}\;\Varid{even}\;\Varid{f}\;\Varid{y}\mathrel{=}\Varid{y}} on odd \ensuremath{\Varid{y}}}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f})\;(\underline{\Varid{set}\;\Varid{n}\;\Varid{xs}}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\Varid{set}}}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{f}\mathbin{\circ}\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})})\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{\ensuremath{\Varid{f}} commutes with \ensuremath{\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})} on input $< 2^n$}
    \stmt{\ensuremath{(\underline{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})}\mathbin{\circ}\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f})\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{Definition of \ensuremath{\Varid{unshift}}}
    \stmt{\ensuremath{(\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}\mathbin{\circ}\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})}\mathbin{\circ}\Varid{f})\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{\ensuremath{not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1})} is false on output of \ensuremath{\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})}}
    \stmt{\ensuremath{(\underline{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})}\mathbin{\circ}\Varid{f})\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
    \reason{=}{\ensuremath{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})} and \ensuremath{\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})} are inverse, since $\ensuremath{\Varid{f}\;\Varid{x}} < 2^{n+1}$}
    \stmt{\ensuremath{\Varid{f}\;(\Varid{xs}\mathrel{:\!.}\Conid{I})}}
  \end{sproof}
  Next, if \ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{O}}, \ensuremath{\Varid{atLSB}\;\Varid{f}\;(\Varid{xs}\mathrel{:\!.}\Conid{O})\mathrel{=}\Varid{atLSB}\;\Varid{f}\;\Varid{xs}\mathrel{:\!.}\Conid{O}}.  For
  the left-hand side, we can compute:
  \begin{sproof}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\underline{\Varid{shift}\;(\Varid{n}\mathbin{+}\mathrm{1})})\;(\Varid{xs}\mathrel{:\!.}\Conid{O})}}
    \reason{=}{Definition of \ensuremath{\Varid{shift}}}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\underline{\Varid{set}\;(\Varid{n}\mathbin{+}\mathrm{1})})\;(\Varid{xs}\mathrel{:\!.}\Conid{O})}}
    \reason{=}{Definition of \ensuremath{\Varid{set}}}
    \stmt{\ensuremath{(\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{while}\;\Varid{even}\;\Varid{shr})\;(\underline{\Varid{set}\;\Varid{n}\;\Varid{xs}\mathrel{:\!.}\Conid{O}})}}
    \reason{=}{Definition of \ensuremath{\Varid{while}} and \ensuremath{\Varid{even}}}
    \stmt{\ensuremath{(\underline{\Varid{unshift}\;(\Varid{n}\mathbin{+}\mathrm{1})}\mathbin{\circ}\Varid{f}\mathbin{\circ}\underline{\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{set}\;\Varid{n}})\;\Varid{xs}}}
    \reason{=}{Definition of \ensuremath{\Varid{unshift}} and \ensuremath{\Varid{shift}}}
    \stmt{\ensuremath{(\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}}\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{shift}\;\Varid{n})\;\Varid{xs}}}
  \end{sproof}
  At this point we would like to rewrite \ensuremath{\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}} by pulling out one iteration of \ensuremath{\Varid{shl}}. Since
  $\ensuremath{\Varid{x}\mathrel{=}\Varid{xs}\mathrel{:\!.}\Conid{O}} < 2^n$, we have $\ensuremath{\Varid{xs}} < 2^{n-1}$ and
  $\ensuremath{\Varid{shift}\;\Varid{n}\;\Varid{xs}} < 2^n + 2^{n-1}$ (recall that \ensuremath{\Varid{shift}\;\Varid{n}\mathrel{=}\Varid{while}\;\Varid{even}\;\Varid{shr}\mathbin{\circ}\Varid{set}\;\Varid{n}} sets the $n$th bit and then can only make the number
  smaller by doing repeated right shifts). Hence by assumption
  $\ensuremath{\Varid{f}\;(\Varid{shift}\;\Varid{n}\;\Varid{xs})} < 2^{n+1}$, and we may apply \pref{lem:shlwhile}.
  \begin{sproof}
    \stmt{\ensuremath{(\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\underline{\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;(\Varid{n}\mathbin{+}\mathrm{1}))\;\Varid{shl}}\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{shift}\;\Varid{n})\;\Varid{xs}}}
    \reason{=}{\pref{lem:shlwhile}}
    \stmt{\ensuremath{(\underline{\Varid{clear}\;(\Varid{n}\mathbin{+}\mathrm{1})\mathbin{\circ}\Varid{shl}}\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;\Varid{n})\;\Varid{shl}\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{shift}\;\Varid{n})\;\Varid{xs}}}
    \reason{=}{\pref{lem:clearshl}}
    \stmt{\ensuremath{(\Varid{shl}\mathbin{\circ}\underline{\Varid{clear}\;\Varid{n}\mathbin{\circ}\Varid{while}\;(not\mathbin{\circ}\Varid{test}\;\Varid{n})\;\Varid{shl}}\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{shift}\;\Varid{n})\;\Varid{xs}}}
    \reason{=}{Definition of \ensuremath{\Varid{unshift}}}
    \stmt{\ensuremath{(\Varid{shl}\mathbin{\circ}\Varid{unshift}\;\Varid{n}\mathbin{\circ}\Varid{f}\mathbin{\circ}\Varid{shift}\;\Varid{n})\;\Varid{xs}}}
    \reason{=}{Induction hypothesis}
    \stmt{\ensuremath{\Varid{shl}\;(\Varid{atLSB}\;\Varid{f}\;\Varid{xs})}}
    \reason{=}{Definition of \ensuremath{\Varid{shl}}}
    \stmt{\ensuremath{\Varid{atLSB}\;\Varid{f}\;\Varid{xs}\mathrel{:\!.}\Conid{O}}}
  \end{sproof}
\vspace{-3\baselineskip}
\end{proof}
\fi

\end{document}
